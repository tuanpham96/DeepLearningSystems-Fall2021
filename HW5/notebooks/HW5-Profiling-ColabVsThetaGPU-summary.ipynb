{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW5-Profiling-ColabVsThetaGPU-summary.ipynb","provenance":[],"collapsed_sections":["AU4zpdGoAMyE","ykWhozVWAXKK","dEY3qewka2rk","3oYZ43p1at9k","PWYLLBStZD4u","7kEF6ism-gcQ","dbWFcEDqlNLE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H0vtdWBYhW_E"},"source":["> In assignment 3, we used a vanilla transformer for Spanish-English Neural Machine Translation. It took quite a long time to train (compared to those smaller models), so we will learn to use a profiler to identify what exactly is the bottlenecks and possible improvements.\n",">\n","> The first part is a quick demo on how to use the TensorFlow Profiler. A couple of post readings will guide you towards different profiler use cases. Then you need to complete the modification to the `Transformer` Class in order to use `.fit()` method. This is required for profiler callbacks during training. After profiling the vanilla transformer, you will describe your profiling results to identify bottlenecks, propose and experiment to reduce or eliminate 3 of these bottlenecks. Discuss your experiment and results. Finally, rerun the non-improved version on ThetaGPU and compare profiling differences, and answer some questions. \n",">\n","> The notebook is at: https://drive.google.com/file/d/1MwaOPAW8xfadGhFlsuziLRf80YfGcOnq/view?usp=sharing\n"]},{"cell_type":"markdown","metadata":{"id":"AWGTq33B8ovZ"},"source":["# Experiments & Write up\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pLHeQmWih1H2"},"source":["### 1. Describe your profiling results on Colab, according to your understanding of this particular transformer architecture design. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W_ssPNOWrugm"},"source":["As expected (see the screenshot pie charts below for `ColabBase[line]`), much of the time was spent on matrix multiplication as the transformer models not only consist of a lot of `matmul` in the rudimentary components (like `dense`) but especially in the attention component. Additionally, for the overview statistics, only very little time was spent on input as there was not much to be done or transformed for the inputs. Around 10% was spent on `Kernel launch time`, one of the places could have been in the `scaled_dot_product_attention` function. On average, 149.4ms / 175.4ms was the `Device compute time` (around 85.1%), which is understandable given the overwhelming number of `matmul` operations. Following `matmul` ops is `transpose`, which is also understandable as that operation appears quite a lot of times in `MultiheadAttention`: each call performs at least 4 operations.  "]},{"cell_type":"code","metadata":{"id":"zDaI-UAfWPOj","colab":{"base_uri":"https://localhost:8080/","height":483},"cellView":"form","executionInfo":{"status":"ok","timestamp":1635987977935,"user_tz":300,"elapsed":156,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"9f579beb-5c65-4965-a0f4-d90054b9e453"},"source":["#@title Overview summary \n","import yaml\n","import pandas as pd\n","results = yaml.safe_load(open('data/mean_time.yml'))\n","pd.DataFrame(results)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ThetaGPUBaseline</th>\n","      <th>ColabBaseline</th>\n","      <th>ColabFlagNCache</th>\n","      <th>ColabMinCast</th>\n","      <th>ColabXLA</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Average Step Time</th>\n","      <td>71.7 ms</td>\n","      <td>175.4 ms</td>\n","      <td>177.0 ms</td>\n","      <td>175.9 ms</td>\n","      <td>176.1 ms</td>\n","    </tr>\n","    <tr>\n","      <th>All Others Time</th>\n","      <td>1.4 ms</td>\n","      <td>4.1 ms</td>\n","      <td>3.9 ms</td>\n","      <td>3.9 ms</td>\n","      <td>4.3 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Compilation Time</th>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Output Time</th>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Input Time</th>\n","      <td>0.1 ms</td>\n","      <td>0.2 ms</td>\n","      <td>0.4 ms</td>\n","      <td>0.6 ms</td>\n","      <td>0.2 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Kernel Launch Time</th>\n","      <td>47.0 ms</td>\n","      <td>18.2 ms</td>\n","      <td>18.5 ms</td>\n","      <td>17.1 ms</td>\n","      <td>18.2 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Host Compute Time</th>\n","      <td>0.9 ms</td>\n","      <td>3.5 ms</td>\n","      <td>4.1 ms</td>\n","      <td>4.5 ms</td>\n","      <td>3.8 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Device Collective Communication Time</th>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Device to Device Time</th>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","      <td>0.0 ms</td>\n","    </tr>\n","    <tr>\n","      <th>Device Compute Time</th>\n","      <td>22.2 ms</td>\n","      <td>149.4 ms</td>\n","      <td>150.0 ms</td>\n","      <td>149.8 ms</td>\n","      <td>149.6 ms</td>\n","    </tr>\n","    <tr>\n","      <th>TF Op Placement - Host</th>\n","      <td>8.0%</td>\n","      <td>4.0%</td>\n","      <td>4.1%</td>\n","      <td>4.1%</td>\n","      <td>0.4%</td>\n","    </tr>\n","    <tr>\n","      <th>TF Op Placement - Device</th>\n","      <td>92.0%</td>\n","      <td>96.0%</td>\n","      <td>95.9%</td>\n","      <td>95.9%</td>\n","      <td>99.6%</td>\n","    </tr>\n","    <tr>\n","      <th>Op Time Spent on Eager Execution - Host</th>\n","      <td>0.1%</td>\n","      <td>34.6%</td>\n","      <td>52.9%</td>\n","      <td>5.5%</td>\n","      <td>83.7%</td>\n","    </tr>\n","    <tr>\n","      <th>Op Time Spent on Eager Execution - Device</th>\n","      <td>0.0%</td>\n","      <td>0.0%</td>\n","      <td>0.0%</td>\n","      <td>0.0%</td>\n","      <td>0.0%</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          ThetaGPUBaseline  ...  ColabXLA\n","Average Step Time                                  71.7 ms  ...  176.1 ms\n","All Others Time                                     1.4 ms  ...    4.3 ms\n","Compilation Time                                    0.0 ms  ...    0.0 ms\n","Output Time                                         0.0 ms  ...    0.0 ms\n","Input Time                                          0.1 ms  ...    0.2 ms\n","Kernel Launch Time                                 47.0 ms  ...   18.2 ms\n","Host Compute Time                                   0.9 ms  ...    3.8 ms\n","Device Collective Communication Time                0.0 ms  ...    0.0 ms\n","Device to Device Time                               0.0 ms  ...    0.0 ms\n","Device Compute Time                                22.2 ms  ...  149.6 ms\n","TF Op Placement - Host                                8.0%  ...      0.4%\n","TF Op Placement - Device                             92.0%  ...     99.6%\n","Op Time Spent on Eager Execution - Host               0.1%  ...     83.7%\n","Op Time Spent on Eager Execution - Device             0.0%  ...      0.0%\n","\n","[14 rows x 5 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"HAcAP4XEZRvg"},"source":["Here's the aggregation of screenshots with the **tensorflow statistics** for different operations.\n","\n","![screen shot](https://raw.githubusercontent.com/tuanpham96/DeepLearningSystems-Fall2021/main/HW5/media/screenshot-tboard-tfstats.png)"]},{"cell_type":"markdown","metadata":{"id":"ndWEUpcSh5u4"},"source":["### 2 + 3. Identify bottlenecks in the training phase using these diagrams. \n","Choose three bottlenecks you think could be improved or eliminated by employing techniques learned from [Debugging and Optimization](https://colab.research.google.com/drive/1MwaOPAW8xfadGhFlsuziLRf80YfGcOnq?authuser=1#scrollTo=9VEjIwHO8Tv9) readings. \n","\n","Carry out experiments (on Colab) testing your modifications to the pipeline / model, discuss:\n","- Why do you think it is an improvable bottleneck?\n","- How do you plan to modify to make it better?\n","- Does your plan work out well? If not, what could be the preventive factor?\n"]},{"cell_type":"markdown","metadata":{"id":"bYQDY2JtiAHQ"},"source":["#### Try 1 - **`ColabFlagNCache`**: Set `TF_GPU_THREAD_MODE=gpu_private` and use `cache` for data\n","\n","As the recommendations from Tensorboard say that there's not much input issue but suggest changing `TF_GPU_THREAD_MODE` to reduce `Kernel launch time`, the first thing I did was to apply this change and also add `cache` for loading the dataset to maximize [recommended optimizations](https://www.tensorflow.org/guide/data_performance) with loading data. Since no transformations were performed during loading dataset (hence doesn't make sense to do parallel calls), and `prefetch` was already applied with `AUTOTUNE`, the only viable thing left was `cache`. \n","\n","Although the % of `Dataset` in `tensorflow_stats` was reduced, no improvement was observed as the average time per step increased by 1.6ms and extended 0.2ms to input time (possibly insignificant). Additionally, `Op Time Spent on Eager Execution` increased by around 18% on `Host`, which was undesirable. \n","\n","No improvement was observed possibly because at least in this scenario the input loading has already been quite optimized already with `prefetch`. For the `TF_GPU_THREAD_MODE` tag, I am unsure why no enhancement on `Kernel launch time` was observed. This was also [observed by other people](https://github.com/tensorflow/profiler/issues/8) was well. "]},{"cell_type":"markdown","metadata":{"id":"AXqDQYXTlRMA"},"source":["#### Try 2 - **`ColabMinCast`**: Minimize `cast` ops"]},{"cell_type":"markdown","metadata":{"id":"NwYtP8pXlcEi"},"source":["Another thing I tried was to look for alternatives for `cast` operations done in the model and function definitions. See below for an example. The changes are in `src/model_mincast.py`. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ml0FUegZlRMB","executionInfo":{"status":"ok","timestamp":1635980547330,"user_tz":300,"elapsed":158,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"4bcf024c-acc3-477a-bed4-fb9fff58b465"},"source":["# this is just one example where trying to replace `cast` op achieves some speedup\n","a = tf.random.normal([10000,100])\n","%timeit -n 100 tf.cast(tf.math.equal(a, 0), tf.float32) # in the original code `src/model.py`\n","%timeit -n 100 tf.where(tf.math.equal(a, 0), 1.0, 0.0)  # changed in `src/model_mincast.py`"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100 loops, best of 5: 127 µs per loop\n","100 loops, best of 5: 87.3 µs per loop\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZRMQbXj2l75h"},"source":["Interesting, this reduced `Kernel launch time` by more than 1ms (though unclear how significant this is) and I do not really have much explanation for this (as I also changed a couple of other things in the code like using `Sequential` to merge a few layers together but I doubt there was much gain from such change). Although this interestingly also reduced `Op Time Spent on Eager Execution - Host` from 34.6% to 5.5% (maybe the explicit casting relied more on the host eager execution), no improvement from the baseline was achieved, as `Host Compute Time` increased instead. \n","\n","Minimizing the `cast` operation did not gain any speedup, potentially because this particular operation was not really present or took much time in the GPU, was it didn't appear in the pie chart. "]},{"cell_type":"markdown","metadata":{"id":"ufrYPeqBovrY"},"source":["#### Try 3 - **`ColabXLA`**: Turn on XLA flags"]},{"cell_type":"markdown","metadata":{"id":"QmmO4APGpklm"},"source":["Lastly, following suggestion from <https://www.tensorflow.org/guide/gpu_performance_analysis> I tried to do `mixed_precision` but that didn't work as the loss (even with the suggested scaled loss and unscaled gradients) was not calculated correctly (plus Colab warned that this GPU was not optimized for it). So I tried turning the XLA tag instead and do explicit compiling for the `scaled_dot_product_attention`, as the [documentation](https://www.tensorflow.org/xla) said that sped up BERT models. \n","\n","``` python\n","os.environ['TF_XLA_FLAGS']='--tf_xla_auto_jit=2'\n","\n","@tf.function(jit_compile=True)\n","def scaled_dot_product_attention(queries, keys, values, mask):\n","    product = tf.matmul(queries, keys, transpose_b=True)\n","    scaled_product = product / tf.math.sqrt(tf.cast(tf.shape(keys)[-1], tf.float32))\n","    scaled_product += (mask * -1e9)\n","    return tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n","```\n","\n","This increased `TF Op Placement - Device` signifcantly to 99.6% but simultaneously increased `Op Time Spent on Eager Execution - Host` pronouncedly to 83.7%. The average time per step was slightly higher but potentially not significantly. This did not lead to improvement, possibly because this GPU device was not optimized for it much or the operations in `scaled_dot_product_attention` did not fuse optimally. "]},{"cell_type":"markdown","metadata":{"id":"NK2Y_VvwrnAt"},"source":["### 4. Run the `unimproved version` notebook on ThetaGPU single-gpu queue, report your finding on profiling result difference. Are those bottlenecks you identified still bottlenecks?"]},{"cell_type":"markdown","metadata":{"id":"gCEiEtFuryoX"},"source":["See the table and pie charts above for the results. \n","\n","The `Device compute time` was significantly improved as ThetaGPU (22.2ms) seems to be much more powerful than Colab (~149-150ms). `Host Compute Time` and `Input time`, though already small, were also more optimal on ThetaGPU than on Colab. `TF Op Placement` percentages were not as optimal as Colab but that doesn't matter much. The `Op Time Spent on Eager Execution` was significantly reduced for both `host` and `device` compared to all Colab versions. \n","\n","The only bottleneck that appears to be on ThetaGPU but not so much on Colab was the `Kernel launch time` (47ms for ThetaGPU vs 17-19ms for Colab). Maybe changing the flags might have ameliorated this. Regardless, since the compute time was significantly reduced, the total average timeper step was much reduced, by nerly 60%. \n","\n","In terms of operation statistics, interestingly `MatMul` and `Argmax` were among the top for ThetaGPU although `MatMul` and `Transpose` were the top 2 for Colab. This is possibly because ThetaGPU devices are optimized for expensive matrix multiplication operations, hence explaining why this supposed bottleneck on Colab (can't get around unless knowing how sparse the matrices are) is easily solved on ThetaGPU. "]},{"cell_type":"markdown","metadata":{"id":"tQA4IpWmwIc4"},"source":["# Source repo\n","<https://github.com/tuanpham96/DeepLearningSystems-Fall2021/tree/main/HW5>"]},{"cell_type":"markdown","metadata":{"id":"AU4zpdGoAMyE"},"source":["# Initialization"]},{"cell_type":"code","metadata":{"id":"lKImH_1CZ7I0"},"source":["%%capture\n","!pip install -U tensorboard-plugin-profile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1ko20uqLNIm","executionInfo":{"status":"ok","timestamp":1635983864024,"user_tz":300,"elapsed":36432,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"ed90c6f4-023a-49fe-d1c0-40793e482994"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd \"/content/drive/MyDrive/Courses/Fall 2021/dlsys/DeepLearningSystems-Fall2021/HW5\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Courses/Fall 2021/dlsys/DeepLearningSystems-Fall2021/HW5\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mo4ugEIsGLhn"},"source":["# Data preparations\n","\n","See see the [data-setup.ipynb](https://github.com/tuanpham96/DeepLearningSystems-Fall2021/tree/main/HW5/notebooks/data-setup.ipynb) notebook"]},{"cell_type":"markdown","metadata":{"id":"YiDvU-K7F322"},"source":["# ThetaGPU\n","See the [HW5-thetagpu.ipynb](https://github.com/tuanpham96/DeepLearningSystems-Fall2021/tree/main/HW5/notebooks/HW5-thetagpu.ipynb) notebook"]},{"cell_type":"markdown","metadata":{"id":"zfVNwDYAATbv"},"source":["# Colab"]},{"cell_type":"markdown","metadata":{"id":"ykWhozVWAXKK"},"source":["## Essentials"]},{"cell_type":"code","metadata":{"id":"HSe4Fr7TTE77"},"source":["from datetime import datetime\n","from src.routines import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRPEJ-XFLxD0"},"source":["# Define paths\n","DATA_PATH = 'data/eng_spa_translations'\n","OUTPUT_PATH = 'output'\n","TRAIN_FILENAME = 'spa.txt'\n","URL_NONBREAKING_FILES = ['nonbreaking_prefix.en', 'nonbreaking_prefix.es']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyYDaXj6Spj9"},"source":["# Define configs\n","data_files = configure_datafiles(\n","    data_path               = DATA_PATH, \n","    train_filename          = TRAIN_FILENAME, \n","    nonbreaking_filenames   = URL_NONBREAKING_FILES\n",")\n","\n","model_config = dict(    \n","    d_model                 = 512,\n","    n_layers                = 4,\n","    FFN_units               = 512,\n","    n_heads                 = 8,\n","    dropout_rate            = 0.1,\n","    act_fun                 = 'relu',\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEY3qewka2rk"},"source":["## Baseline"]},{"cell_type":"code","metadata":{"id":"b7nLmIAOTrwp"},"source":["# Load and tranform data \n","dataset, token_dset = load_datasets(data_files) \n","\n","# Clean the session\n","tf.keras.backend.clear_session()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKcTl9j7a6JO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635979614554,"user_tz":300,"elapsed":449116,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"7c7a9cd4-810f-4a4a-b7fd-1ba53709347e"},"source":["# Model name \n","model_name = 'transformer-ColabBaseline'\n","# Create model\n","transformer = Transformer(\n","    vocab_size_enc=token_dset['input']['num_words'], \n","    vocab_size_dec=token_dset['target']['num_words'],\n","    **model_config\n",")\n","# Compile model \n","compile_model(transformer, model_config)\n","# Fit with callbacks\n","fit_model_with_callbacks(transformer, dataset, model_name, num_epochs=2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","1250/1250 [==============================] - 228s 172ms/step - loss: 2.5786 - train_accuracy: 0.1805\n","Epoch 2/2\n","1250/1250 [==============================] - 210s 168ms/step - loss: 1.5255 - train_accuracy: 0.2731\n"]}]},{"cell_type":"markdown","metadata":{"id":"3oYZ43p1at9k"},"source":["## Try 1: Set `TF_GPU_THREAD_MODE=gpu_private` and use `cache` for data\n"]},{"cell_type":"code","metadata":{"id":"USOjHLhWdPCM"},"source":["os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private' # Change flag \n","dataset, token_dset = load_datasets(data_files, use_cache=True) # Use flag \n","tf.keras.backend.clear_session()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8Hxx0dua_vr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635980130541,"user_tz":300,"elapsed":442527,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"e17c51e4-ac00-434f-cbda-e8d607a969f4"},"source":["# Model name \n","model_name = 'transformer-ColabFlagNCache'\n","# Create model\n","transformer = Transformer(\n","    vocab_size_enc=token_dset['input']['num_words'], \n","    vocab_size_dec=token_dset['target']['num_words'],\n","    **model_config\n",")\n","# Compile model \n","compile_model(transformer, model_config)\n","# Fit with callbacks\n","fit_model_with_callbacks(transformer, dataset, model_name, num_epochs=2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","1250/1250 [==============================] - 223s 171ms/step - loss: 2.5767 - train_accuracy: 0.1805\n","Epoch 2/2\n","1250/1250 [==============================] - 209s 167ms/step - loss: 1.5351 - train_accuracy: 0.2717\n"]}]},{"cell_type":"markdown","metadata":{"id":"PWYLLBStZD4u"},"source":["## Try 2: Minimize `cast` ops"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DA26MXEKwKF5","executionInfo":{"status":"ok","timestamp":1635980547330,"user_tz":300,"elapsed":158,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"4bcf024c-acc3-477a-bed4-fb9fff58b465"},"source":["# this is just one example where trying to replace `cast` op achieves some speedup\n","a = tf.random.normal([10000,100])\n","%timeit -n 100 tf.cast(tf.math.equal(a, 0), tf.float32)\n","%timeit -n 100 tf.where(tf.math.equal(a, 0), 1.0, 0.0)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100 loops, best of 5: 127 µs per loop\n","100 loops, best of 5: 87.3 µs per loop\n"]}]},{"cell_type":"code","metadata":{"id":"1z_DFK3e_6H4"},"source":["from src.model_mincast import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVORRGZt__CT","executionInfo":{"status":"ok","timestamp":1635981196613,"user_tz":300,"elapsed":490722,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"a66ec9f1-b071-4e59-dfc4-21903f6fbf22"},"source":["# Model name \n","model_name = 'transformer-ColabMinCast'\n","# Create model\n","transformer = Transformer(\n","    vocab_size_enc=token_dset['input']['num_words'], \n","    vocab_size_dec=token_dset['target']['num_words'],\n","    **model_config\n",")\n","# Compile model \n","compile_model(transformer, model_config)\n","# Fit with callbacks\n","fit_model_with_callbacks(transformer, dataset, model_name, num_epochs=2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","1250/1250 [==============================] - 228s 173ms/step - loss: 2.5716 - train_accuracy: 0.1809\n","Epoch 2/2\n","1250/1250 [==============================] - 213s 170ms/step - loss: 1.5422 - train_accuracy: 0.2703\n"]}]},{"cell_type":"markdown","metadata":{"id":"7kEF6ism-gcQ"},"source":["## Try 3: Turn on `XLA` flags"]},{"cell_type":"code","metadata":{"id":"ya10yBqmo8qF"},"source":["os.environ['TF_XLA_FLAGS']='--tf_xla_auto_jit=2'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOmeHUz9FGTZ"},"source":["# reload the baseline model definitions instead of the mincast version\n","from src.model import * "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"it6CSO8UD1n2"},"source":["del scaled_dot_product_attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiRNR8w3D0wO"},"source":["@tf.function(jit_compile=True)\n","def scaled_dot_product_attention(queries, keys, values, mask):\n","    product = tf.matmul(queries, keys, transpose_b=True)\n","    scaled_product = product / tf.math.sqrt(tf.cast(tf.shape(keys)[-1], tf.float32))\n","    scaled_product += (mask * -1e9)\n","    return tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgkPWkTNDvui","executionInfo":{"status":"ok","timestamp":1635981744372,"user_tz":300,"elapsed":454875,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"e6e0e272-434f-4b94-a1d6-bc4d4b72b84f"},"source":["# Model name \n","model_name = 'transformer-ColabXLA'\n","# Create model\n","transformer = Transformer(\n","    vocab_size_enc=token_dset['input']['num_words'], \n","    vocab_size_dec=token_dset['target']['num_words'],\n","    **model_config\n",")\n","# Compile model \n","compile_model(transformer, model_config)\n","# Fit with callbacks\n","fit_model_with_callbacks(transformer, dataset, model_name, num_epochs=2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","1250/1250 [==============================] - 231s 175ms/step - loss: 2.5767 - train_accuracy: 0.1808\n","Epoch 2/2\n","1250/1250 [==============================] - 214s 171ms/step - loss: 1.5201 - train_accuracy: 0.2745\n"]}]},{"cell_type":"markdown","metadata":{"id":"dbWFcEDqlNLE"},"source":["# Tensorboard"]},{"cell_type":"code","metadata":{"id":"p7G1ki8fTMPZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635990094594,"user_tz":300,"elapsed":214,"user":{"displayName":"Tuan Pham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgG9io_G8UlUCnCJIPDToyzRUEWyAQYHjYGEIZsiA=s64","userId":"04312096117679155400"}},"outputId":"a960b97a-193e-4032-c734-f9217f37596b"},"source":["# Load the TensorBoard notebook extension.\n","%load_ext tensorboard"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}]},{"cell_type":"code","metadata":{"id":"BLo6p0D_c2yr"},"source":["# If needed to kill and reload\n","!ps aux | grep '[/]bin/tensorboard' | awk '{print $2}' | xargs kill\n","%reload_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEx50PU4TNSN"},"source":["# Launch TensorBoard and navigate to the Profile tab to view performance profile\n","# in order to save with version control, not gonna save this as it makes this notebook 12M\n","# see the above summary instead\n","%tensorboard --logdir=logs"],"execution_count":null,"outputs":[]}]}