{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 2 - CNN & ResNet","provenance":[{"file_id":"1kyFRtM70oZ28ERx4ey5Qd4aLw_AR7gRx","timestamp":1633715317365}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ENFSh4IaGkcB"},"source":["In this assignment, you will experiment with different model components and architectures to design a classifier on CIFAR10 dataset."]},{"cell_type":"markdown","metadata":{"id":"h7M2LIWpCSWr"},"source":["We have been talking about CNN and ResNet structures and some of their components details over the last week. In assignment 2, you will have the experience to learn how to create a customized dataset and to study how different choice of model components and architectures affects its performance. Performance measurement will be evaluated with respect to trainability, accuracy, speed. A basic CNN model is provided to you, available in both Pytorch and Tensorflow. The provided codes are replicated to the maximum level. \n","\n","You will be experiment with:\n","\n","- activation functions\n","- skip connection\n","- batch normalization\n","- optimizers\n","- learning rate (schedule)\n","\n","\n","A challenge question on \"whether skip connection alone provides better - performance, without retraining?\" is presented as an optional exploration, in response to the discussion during the ResNet lecture.\n","\n","The notebook is at https://colab.research.google.com/drive/1kyFRtM70oZ28ERx4ey5Qd4aLw_AR7gRx?usp=sharing (Links to an external site.). Choose either Tensorflow or Pytorch for this assignment. Please neatly organize the write-up in the provided section. Start early on assignments from now on, as it will consume more time than assignment 1 did.\n","\n","Submit your assignment through this form: https://forms.gle/uuzeRgbBzfsrtd7s9 (Links to an external site.). After submission, you can peer review the works from your classmates. The auto-updated timestamp will be used to determine late submission. 20% deducted if late. DO NOT VIOLATE THE ACADEMIC CODE OF CONDUCT. "]},{"cell_type":"markdown","metadata":{"id":"_uMikSFlmS-l"},"source":["# Write Up -- General questions\n","[click to edit]"]},{"cell_type":"markdown","metadata":{"id":"oX8J9cUIrYfu"},"source":["# Write Up -- Challenge question\n","[click to edit]\n"]},{"cell_type":"markdown","metadata":{"id":"LweTw8wQnj9b"},"source":["# Tasks [choose **GPU** with **either Pytorch or Tensorflow**]"]},{"cell_type":"markdown","metadata":{"id":"wXF9Eo5jR53d"},"source":["## General Explorations\n","In assignment 1, you had the experience of running and benchmarking the same CNN model with different combinations of platforms and hardware choices. Now in assignment 2, we would like you to start with the provided base model and tweak it with different components and structures. Please provide a writeup describing your findings of the following items:\n","\n","- Effects on **trainability**: Fixed initial states, fixed total iterations, does it converge on each setting, fast or slow?\n","- Effects on **accuracy**: Fixed initial states, fixed total iterations, what's the test accuracy for each setting? \n","- Effects on **speed (samples / second)**: What's the speed for 1 training epoch? What's the speed for 1 inference(test) epoch?\n","\n","## Activation functions:\n","[Wiki](https://en.wikipedia.org/wiki/Activation_function#Sign_equivalence_to_identity_function)\n","[Pytorch](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n","[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n","\n","_\\* for simplicity, assume all activation functions within a network to be the same_\n","1. Sigmoid\n","2. TanH\n","3. ReLU\n","4. ELU\n","5. Leaky ReLU\n","6. Swish (SiLU)\n","\n","## Skip connections:\n","[Pytorch & Tensorflow have quite similar style nowadays](https://discuss.pytorch.org/t/how-to-write-your-own-skip-connections-in-pytorch/88618)\n","- no skip\n","- skip 1 layer, trainable parameters\n","- skip 2 layers, trainable parameters\n","\n","## Batch normalization:\n","[Pytorch](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n","[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)\n","\n","- on vs. off\n","- when: before vs. after activation?\n","- where: uniformly apply vs. only after certain layers?\n","\n","## Optimizers:\n","[Pytorch](https://pytorch.org/docs/stable/optim.html#algorithms)\n","[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n","1. SGD\n","2. Adam\n","\n","## Learning rate (scheduler):\n","[Pytorch](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n","[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)\n","- constant learning rate\n","- step decay / exponential decay\n","- ReduceOnPlateau\n","- Annealing Schedule"]},{"cell_type":"markdown","metadata":{"id":"PRfJ1wJlkcTE"},"source":["## Challenge Question: does skip connection alone provides better performance without re-training? [Optional]\n","bring your best model, freeze all its parameters, add skip connections, does it improve on test accuracy?\n","\n","How to do this:\n","- save the trained model parameters\n","- add skip connection in code of model definition [how to](https://discuss.pytorch.org/t/how-to-write-your-own-skip-connections-in-pytorch/88618)\n","- reload the parameters\n","- start inference"]},{"cell_type":"markdown","metadata":{"id":"7X9SeBUBWcSF"},"source":["# Set up storage and download dataset"]},{"cell_type":"markdown","metadata":{"id":"SlU-gEiGzSVC"},"source":["## Connect to google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uk48pYb1GSCo","executionInfo":{"status":"ok","timestamp":1633671137686,"user_tz":300,"elapsed":15461,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"56f031a4-f9c6-408c-bba7-8a2247550264"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"mMaPX_HSSnOA"},"source":["## What do we use for dataset?\n","We will be using CIFAR10 and some of its derivatives for this assignment. Following descripions are from tensorflow dataset website.\n","\n","- The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. This serves as our training set\\\n","Homepage: https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","- The CIFAR-10.1 dataset is a new test set for CIFAR-10. CIFAR-10.1 contains roughly 2,000 new test images that were sampled after multiple years of research on the original CIFAR-10 dataset. The data collection for CIFAR-10.1 was designed to minimize distribution shift relative to the original dataset. We describe the creation of CIFAR-10.1 in the paper \"Do CIFAR-10 Classifiers Generalize to CIFAR-10?\". The images in CIFAR-10.1 are a subset of the TinyImages dataset. There are currently two versions of the CIFAR-10.1 dataset: v4 and v6. **We use v6 for this assignment.** \\\n","Homepage: https://github.com/modestyachts/CIFAR-10.1\n","\n","To prepare for the future assignments & projects, we will load datasets by manual efforts to mimic the data loading process in most of the real world DL experiments. For storage, we will use Google Drive.\n"]},{"cell_type":"code","metadata":{"id":"_0FlFQx3bjOE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633671226829,"user_tz":300,"elapsed":8602,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"cdedd116-8e5d-4054-8100-d7de924eaa70"},"source":["# Download CIFAR10 & CIFAR10.1 v6\n","!mkdir -p \"/content/drive/MyDrive/Deep Learning System/assignment-2/data/\"\n","%cd \"/content/drive/MyDrive/Deep Learning System/assignment-2/data/\"\n","!wget \"https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\" -O \"cifar-10-binary.tar.gz\"\n","!wget \"https://github.com/modestyachts/CIFAR-10.1/raw/master/datasets/cifar10.1_v6_data.npy\" -O \"cifar10.1_v6_data.npy\"\n","!wget \"https://github.com/modestyachts/CIFAR-10.1/raw/master/datasets/cifar10.1_v6_labels.npy\" -O \"cifar10.1_v6_labels.npy\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep Learning System/assignment-2/data\n","--2021-10-08 05:33:39--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170052171 (162M) [application/x-gzip]\n","Saving to: ‘cifar-10-binary.tar.gz’\n","\n","cifar-10-binary.tar 100%[===================>] 162.17M  38.3MB/s    in 4.4s    \n","\n","2021-10-08 05:33:44 (36.5 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n","\n","--2021-10-08 05:33:44--  https://github.com/modestyachts/CIFAR-10.1/raw/master/datasets/cifar10.1_v6_data.npy\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/modestyachts/CIFAR-10.1/master/datasets/cifar10.1_v6_data.npy [following]\n","--2021-10-08 05:33:45--  https://raw.githubusercontent.com/modestyachts/CIFAR-10.1/master/datasets/cifar10.1_v6_data.npy\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6144128 (5.9M) [application/octet-stream]\n","Saving to: ‘cifar10.1_v6_data.npy’\n","\n","cifar10.1_v6_data.n 100%[===================>]   5.86M  34.1MB/s    in 0.2s    \n","\n","2021-10-08 05:33:45 (34.1 MB/s) - ‘cifar10.1_v6_data.npy’ saved [6144128/6144128]\n","\n","--2021-10-08 05:33:45--  https://github.com/modestyachts/CIFAR-10.1/raw/master/datasets/cifar10.1_v6_labels.npy\n","Resolving github.com (github.com)... 140.82.114.4\n","Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/modestyachts/CIFAR-10.1/master/datasets/cifar10.1_v6_labels.npy [following]\n","--2021-10-08 05:33:45--  https://raw.githubusercontent.com/modestyachts/CIFAR-10.1/master/datasets/cifar10.1_v6_labels.npy\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8128 (7.9K) [application/octet-stream]\n","Saving to: ‘cifar10.1_v6_labels.npy’\n","\n","cifar10.1_v6_labels 100%[===================>]   7.94K  --.-KB/s    in 0.001s  \n","\n","2021-10-08 05:33:45 (11.6 MB/s) - ‘cifar10.1_v6_labels.npy’ saved [8128/8128]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYYGFG6IguVb","executionInfo":{"status":"ok","timestamp":1633671246991,"user_tz":300,"elapsed":6467,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"7680b114-de97-4900-907e-2982cfdca163"},"source":["# decompress the tar.gz. It will create a folder `cifar-10-batches-bin/` under `/content/drive/MyDrive/Deep Learning System/assignment-2/data/`\n","!tar -xvzf \"/content/drive/MyDrive/Deep Learning System/assignment-2/data/cifar-10-binary.tar.gz\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cifar-10-batches-bin/\n","cifar-10-batches-bin/data_batch_1.bin\n","cifar-10-batches-bin/batches.meta.txt\n","cifar-10-batches-bin/data_batch_3.bin\n","cifar-10-batches-bin/data_batch_4.bin\n","cifar-10-batches-bin/test_batch.bin\n","cifar-10-batches-bin/readme.html\n","cifar-10-batches-bin/data_batch_5.bin\n","cifar-10-batches-bin/data_batch_2.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"hP_JcmgCmmbC"},"source":["# Pytorch Script"]},{"cell_type":"code","metadata":{"id":"m_z2EN1Uirjz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633642398850,"user_tz":300,"elapsed":326131,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"cb84f4c2-2c30-47b4-e1bd-29a2b6194aab"},"source":["# pytorch packages setup\n","!pip3 install --no-cache-dir torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.9.1+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n","\u001b[K     |████████████████████████████████| 2041.3 MB 1.3 MB/s \n","\u001b[?25hCollecting torchvision==0.10.1+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (20.6 MB)\n","\u001b[K     |████████████████████████████████| 20.6 MB 658 kB/s \n","\u001b[?25hCollecting torchaudio==0.9.1\n","  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1+cu111) (3.7.4.3)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (1.19.5)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu111\n","    Uninstalling torch-1.9.0+cu111:\n","      Successfully uninstalled torch-1.9.0+cu111\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu111\n","    Uninstalling torchvision-0.10.0+cu111:\n","      Successfully uninstalled torchvision-0.10.0+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.9.1+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.1+cu111 torchaudio-0.9.1 torchvision-0.10.1+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"kLODgGUGIy9-"},"source":["## Environment setup"]},{"cell_type":"code","metadata":{"id":"BVfBZwj1I8JS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633671371971,"user_tz":300,"elapsed":70,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"d11eeb2b-a22e-454f-950e-8a74921de0eb"},"source":["import os\n","import random\n","import numpy as np\n","import torch\n","\n","# common variables\n","BATCH_SIZE = 32\n","EPOCHS = 15\n","SEED = 3456\n","\n","# fix random seed\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd68c11f790>"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"sgSX8531nEgt"},"source":["## Parse and build customized dataset\n","\n","Data format - Binary version: https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","The binary version contains the files data_batch_1.bin, data_batch_2.bin, ..., data_batch_5.bin, as well as test_batch.bin. Each of these files is formatted as follows:\n","```\n","<1 x label><3072 x pixel>\n","...\n","<1 x label><3072 x pixel>\n","```\n","In other words, the first byte is the label of the first image, which is a number in the range 0-9. The next 3072 bytes are the values of the pixels of the image. The first 1024 bytes are the red channel values, the next 1024 the green, and the final 1024 the blue. The values are stored in row-major order, so the first 32 bytes are the red channel values of the first row of the image.\n","\n","Each file contains 10000 such 3073-byte \"rows\" of images, although there is nothing delimiting the rows. Therefore each file should be exactly 30730000 bytes long.\n","\n","There is another file, called batches.meta.txt. This is an ASCII file that maps numeric labels in the range 0-9 to meaningful class names. It is merely a list of the 10 class names, one per row. The class name on row i corresponds to numeric label i."]},{"cell_type":"markdown","metadata":{"id":"VPWajm5XgCgN"},"source":["###  Pytorch Dataset Object <a name=\"torch-dataset\"></a>\n","\n","https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"]},{"cell_type":"code","metadata":{"id":"7K32W0keuimK"},"source":["import itertools\n","import torch.nn.functional as F\n","\n","from pathlib import Path\n","from PIL import Image\n","from torch import nn\n","from torch.utils.data.dataset import Dataset\n","\n","class Cifar35200(Dataset):\n","    def __init__(self, split: str, data_path: str, transform=None, target_transform=None):\n","        \n","        # split: train, valid, test\n","        self.split = split\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","        data_path = Path(data_path)\n","        train_files = [f'data_batch_{i}.bin' for i in range(1,6)]\n","        valid_files = ['test_batch.bin']\n","        \n","        cifar_path = data_path / 'cifar-10-batches-bin'\n","        self.train_paths = [cifar_path / f for f in train_files]\n","        self.valid_paths = [cifar_path / f for f in valid_files]\n","\n","        self.test_labels_path = data_path / 'cifar10.1_v6_labels.npy'\n","        self.test_images_path = data_path / 'cifar10.1_v6_data.npy'\n","\n","        label_path = cifar_path / 'batches.meta.txt'\n","        label_names = [name for name in label_path.read_text().split('\\n') if name]\n","        self.classes = label_names\n","\n","        if self.split == 'test':\n","            images = np.load(self.test_images_path)\n","            labels = np.load(self.test_labels_path)\n","            self.data = tuple(zip(labels, images))\n","        elif self.split == 'valid':\n","            # https://stackoverflow.com/a/41289772\n","            shards = (tuple(self._load_data(p)) for p in self.valid_paths)\n","            self.data = list(itertools.chain.from_iterable(shards))\n","        else:\n","            shards = (tuple(self._load_data(p)) for p in self.train_paths)\n","            self.data = list(itertools.chain.from_iterable(shards))\n","\n","    def _load_data(self, path):\n","        path = Path(path)\n","        data = path.read_bytes()\n","        offset = 0\n","        max_offset = len(data) - 1\n","        while offset < max_offset:\n","            labels = np.frombuffer(data, dtype=np.uint8, count=1, offset=offset).squeeze()\n","            offset += 1\n","            img = (np.frombuffer(data, dtype=np.uint8, count=3072, offset=offset).reshape((3, 32, 32)).transpose((1 ,2, 0)))\n","            offset += 3072        \n","            yield labels, img\n","\n","    def __getitem__(self, index):\n","        label, img = self.data[index]\n","        img = Image.fromarray(img)\n","        label = int(label)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            label = self.target_transform(label)\n","        return (img, label)\n","    \n","    def __len__(self):\n","        count = len(self.data)\n","        return count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ROLrrXfOfpl","executionInfo":{"status":"ok","timestamp":1633673312782,"user_tz":300,"elapsed":91,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"2e7cc818-edd1-4d85-a65b-8105e4b2ff76"},"source":["# mini test\n","ds = Cifar35200('valid', '/content/drive/MyDrive/Deep Learning System/assignment-2/data')\n","len(ds)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"9HVj7eg3Xk3J"},"source":["### Dataset Preprocessing via Transform"]},{"cell_type":"code","metadata":{"id":"4tOoccWTPt8n"},"source":["from torchvision import transforms\n","\n","## transformations\n","transform_train = transforms.Compose([transforms.Resize((32,32)),  #resises the image so it can be perfect for our model.\n","                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n","                                      transforms.RandomRotation(10),     #Rotates the image to a specified angel\n","                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n","                                      transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize all the images\n","                                      ])\n"," \n","transform = transforms.Compose([transforms.Resize((32,32)),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","                               ])\n","\n","data_path = '/content/drive/MyDrive/Deep Learning System/assignment-2/data'\n","\n","## download and load training dataset\n","trainset = Cifar35200('train', data_path, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","validset = Cifar35200('valid', data_path, transform=transform)\n","validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,\n","                                          shuffle=False, num_workers=2)\n","\n","testset = Cifar35200('test', data_path, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                          shuffle=False, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHGPoAARf8S-"},"source":["## Define Model"]},{"cell_type":"code","metadata":{"id":"oMxNcf_aIQ3T"},"source":["class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, 3, padding='valid')\n","        self.conv2 = nn.Conv2d(16, 32, 3, padding='valid')\n","        self.conv3 = nn.Conv2d(32, 64, 3, padding='valid')\n","        self.fc1 = nn.Linear(2*2*64, 500)\n","        self.maxpool = nn.MaxPool2d(2, 2)\n","        self.dropout1 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.maxpool(x)\n","        x = F.relu(self.conv3(x))\n","        x = self.maxpool(x)\n","        # x = x.view(-1, 4*4*64) # flatten our images to 1D to input it to the fully connected layers\n","        x = torch.flatten(x, start_dim=1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout1(x) # Applying dropout b/t layers which exchange highest parameters. This is a good practice\n","        logits = self.fc2(x)\n","        out = F.softmax(logits, dim=1)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThJMP39lSy-F"},"source":["model = LeNet()\n","\n","# pytorch by default use Kaiming Uniform; Tensorflow uses Glorot/Xavier Uniform\n","# https://discuss.pytorch.org/t/crossentropyloss-expected-object-of-type-torch-longtensor/28683/6\n","def weight_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n","        nn.init.zeros_(m.bias)\n","\n","model.apply(weight_init) # apply Glorot/Xavier initialization\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gN7dK9gugCA7"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"V9_-yHCLU0Yr"},"source":["learning_rate = 0.001\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNWHItjeciQP"},"source":["## compute accuracy\n","def get_accuracy(logit, target, batch_size):\n","    ''' Obtain accuracy for training round '''\n","    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","    accuracy = 100.0 * corrects/batch_size\n","    return accuracy.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8ynNFggU9I8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674271647,"user_tz":300,"elapsed":875880,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"b32e2257-bd3f-4e1a-9af7-0cb1ff7860ff"},"source":["from tqdm import tqdm\n","\n","running_loss_history = []\n","running_corrects_history = []\n","val_running_loss_history = []\n","val_running_corrects_history = []\n","\n","for epoch in range(EPOCHS):\n","    train_running_loss = 0.0\n","    train_acc = 0.0\n","    valid_running_loss = 0.0\n","    valid_acc = 0.0\n","\n","    model = model.train()\n","\n","    ## training step\n","    for images, labels in tqdm(trainloader):\n","        \n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        ## forward + backprop + loss\n","        logits = model(images)\n","        loss = criterion(logits, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        ## update model params\n","        optimizer.step()\n","\n","        train_running_loss += loss.detach().item()\n","        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in validloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            logits = model(images)\n","            loss = criterion(logits, labels)\n","            valid_running_loss += loss.detach().item()\n","            valid_acc += get_accuracy(logits, labels, BATCH_SIZE)\n","\n","    print('Epoch: %d | Train Loss: %.4f | Train Accuracy: %.4f | Validation Loss: %.4f | Validation Accuracy: %.4f' \\\n","          %(epoch, train_running_loss / len(trainloader), train_acc / len(trainloader), valid_running_loss / len(validloader), valid_acc / len(validloader)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:53<00:00, 29.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Train Loss: 2.0960 | Train Accuracy: 35.7286 | Validation Loss: 2.0134 | Validation Accuracy: 44.2592\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:53<00:00, 29.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Train Loss: 2.0039 | Train Accuracy: 45.1156 | Validation Loss: 1.9584 | Validation Accuracy: 49.9101\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:53<00:00, 28.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Train Loss: 1.9633 | Train Accuracy: 49.3402 | Validation Loss: 1.9238 | Validation Accuracy: 53.4145\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 | Train Loss: 1.9409 | Train Accuracy: 51.6315 | Validation Loss: 1.9172 | Validation Accuracy: 54.0136\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 4 | Train Loss: 1.9241 | Train Accuracy: 53.3969 | Validation Loss: 1.8923 | Validation Accuracy: 56.5595\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 5 | Train Loss: 1.9196 | Train Accuracy: 53.7888 | Validation Loss: 1.8936 | Validation Accuracy: 56.3998\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 6 | Train Loss: 1.9089 | Train Accuracy: 54.9584 | Validation Loss: 1.8947 | Validation Accuracy: 56.2800\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 7 | Train Loss: 1.9024 | Train Accuracy: 55.5742 | Validation Loss: 1.8675 | Validation Accuracy: 58.8958\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 8 | Train Loss: 1.8988 | Train Accuracy: 55.9841 | Validation Loss: 1.9046 | Validation Accuracy: 55.3115\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 9 | Train Loss: 1.9015 | Train Accuracy: 55.7362 | Validation Loss: 1.8806 | Validation Accuracy: 57.7376\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 10 | Train Loss: 1.9022 | Train Accuracy: 55.7282 | Validation Loss: 1.8864 | Validation Accuracy: 57.2185\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 11 | Train Loss: 1.9016 | Train Accuracy: 55.7782 | Validation Loss: 1.8777 | Validation Accuracy: 58.0871\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 12 | Train Loss: 1.8960 | Train Accuracy: 56.3080 | Validation Loss: 1.8955 | Validation Accuracy: 56.2600\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:54<00:00, 28.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 13 | Train Loss: 1.9008 | Train Accuracy: 55.9001 | Validation Loss: 1.8791 | Validation Accuracy: 58.0471\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:53<00:00, 28.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 14 | Train Loss: 1.8988 | Train Accuracy: 56.0841 | Validation Loss: 1.8722 | Validation Accuracy: 58.7260\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ewm7_apBgGeB"},"source":["## Test score"]},{"cell_type":"code","metadata":{"id":"9bGZ2me7fxU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674450359,"user_tz":300,"elapsed":1014,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"3b6d4077-e0db-4fbe-b2ad-d9dd39065dd4"},"source":["test_acc = 0.0\n","\n","model.eval()\n","with torch.no_grad():\n","    for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        logits = model(images)\n","        test_acc += get_accuracy(logits, labels, BATCH_SIZE)\n","print('Test Accuracy: %.4f'%(test_acc / len(testloader)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 45.9325\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZnxRVQsfmphb"},"source":["# Tensorflow Script"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T__xbLTtz5DN","executionInfo":{"status":"ok","timestamp":1633674696317,"user_tz":300,"elapsed":7504,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"48ef2fb0-d057-401a-fafa-34bf787bb6d7"},"source":["# tensorflow packages setup\n","!pip3 install --no-cache-dir tensorflow==2.6.0 tensorflow-datasets==4.4.0 tensorflow_addons==0.14"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.6.0 in /usr/local/lib/python3.7/dist-packages (2.6.0)\n","Collecting tensorflow-datasets==4.4.0\n","  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 5.2 MB/s \n","\u001b[?25hCollecting tensorflow_addons==0.14\n","  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 33.8 MB/s \n","\u001b[?25hRequirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (5.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.7.4.3)\n","Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (2.6.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.3.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.1.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.12.1)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (2.6.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.15.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.12.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.19.5)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.41.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.37.0)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (2.6.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (3.17.3)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.6.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.2.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.1.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (0.4.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.1.2)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.0) (1.12)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (0.3.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (2.23.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (2.3)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (21.2.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (5.2.2)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (1.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (4.62.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons==0.14) (2.7.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.6.0) (1.5.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2021.5.30)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.6.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.8.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (4.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (3.6.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets==4.4.0) (1.53.0)\n","Installing collected packages: tensorflow-datasets, tensorflow-addons\n","  Attempting uninstall: tensorflow-datasets\n","    Found existing installation: tensorflow-datasets 4.0.1\n","    Uninstalling tensorflow-datasets-4.0.1:\n","      Successfully uninstalled tensorflow-datasets-4.0.1\n","Successfully installed tensorflow-addons-0.14.0 tensorflow-datasets-4.4.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"h6Nj4QkdWRi_"},"source":["## Environment setup"]},{"cell_type":"code","metadata":{"id":"JkZeL8pNWRjF"},"source":["import os\n","import random\n","import numpy as np\n","import tensorflow as tf\n","\n","# common variables\n","BATCH_SIZE = 32\n","EPOCHS = 15\n","SEED = 3456\n","\n","# fix random seed\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_oN85I19XDy0"},"source":["## Parse and build customized dataset\n","\n","Data format - Binary version: https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","The binary version contains the files data_batch_1.bin, data_batch_2.bin, ..., data_batch_5.bin, as well as test_batch.bin. Each of these files is formatted as follows:\n","```\n","<1 x label><3072 x pixel>\n","...\n","<1 x label><3072 x pixel>\n","```\n","In other words, the first byte is the label of the first image, which is a number in the range 0-9. The next 3072 bytes are the values of the pixels of the image. The first 1024 bytes are the red channel values, the next 1024 the green, and the final 1024 the blue. The values are stored in row-major order, so the first 32 bytes are the red channel values of the first row of the image.\n","\n","Each file contains 10000 such 3073-byte \"rows\" of images, although there is nothing delimiting the rows. Therefore each file should be exactly 30730000 bytes long.\n","\n","There is another file, called batches.meta.txt. This is an ASCII file that maps numeric labels in the range 0-9 to meaningful class names. It is merely a list of the 10 class names, one per row. The class name on row i corresponds to numeric label i."]},{"cell_type":"markdown","metadata":{"id":"FZPVg1Wws35y"},"source":["### Tensorflow Dataset Object <a name=\"tf-dataset\"></a>\n","\n","https://www.tensorflow.org/datasets/add_dataset\n","\n","TF2.0 Dataset structure looks very similar to its Pytorch counterpart."]},{"cell_type":"code","metadata":{"id":"Xhx_yhfo8w22","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674664625,"user_tz":300,"elapsed":432,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"77b3ffeb-cc3e-4e42-d100-b89b32851519"},"source":["!mkdir -p /content/drive/MyDrive/Deep Learning System/assignment-2/data/\n","%cd /content/drive/MyDrive/Deep Learning System/assignment-2/data/\n","\n","![ ! -d \"CIFAR35200\" ] && tfds new CIFAR35200"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep Learning System/assignment-2/data\n"]}]},{"cell_type":"markdown","metadata":{"id":"YQ2tbpDGSQHP"},"source":["This will create a folder named \"CIFAR35200\" in `/content/drive/MyDrive/Deep Learning System/assignment-2/data/`. Edit the `CIFAR35200.py` template to define the dataset behavior. For your convenience, here is my `CIFAR35200.py`: https://drive.google.com/file/d/16cIRqrYt6DBBTPru2Dt6ghDpXq4ACZMa/view?usp=sharing \\\n","Please be aware that I changed the file extension to `*.txt` to make it possible for `wget` to work properly. You should restore it to `*.py` if you replicate the changes manually.\n","\n","Let's build this dataset."]},{"cell_type":"code","metadata":{"id":"Ed8s4DM4QVxY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674770120,"user_tz":300,"elapsed":73806,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"e5b1d021-7d05-476d-c992-49a190a3cff4"},"source":["![ -d \"/root/tensorflow_datasets/cifar35200/\" ] && rm /root/tensorflow_datasets/cifar35200/ -r\n","\n","# build should perform in the \"CIFAR35200\" template folder\n","%cd /content/drive/MyDrive/Deep Learning System/assignment-2/data/CIFAR35200\n","\n","# here we directly download the provided CIFAR35200.py. Take some time to study it\n","# https://bcrf.biochem.wisc.edu/2021/02/05/download-google-drive-files-using-wget/\n","!wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=16cIRqrYt6DBBTPru2Dt6ghDpXq4ACZMa\" -O CIFAR35200.py\n","\n","# --manual_dir should point to the directory where your extacted dataset locates\n","!tfds build --manual_dir=\"/content/drive/MyDrive/Deep Learning System/assignment-2/data\"\n","\n","# you will see a test run through the dataset, log ended with dataset info"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Deep Learning System/assignment-2/data/CIFAR35200\n","--2021-10-08 06:31:35--  https://docs.google.com/uc?export=download&id=16cIRqrYt6DBBTPru2Dt6ghDpXq4ACZMa\n","Resolving docs.google.com (docs.google.com)... 209.85.145.138, 209.85.145.100, 209.85.145.113, ...\n","Connecting to docs.google.com (docs.google.com)|209.85.145.138|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0k-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tgs86na8omrrcpv93tc86kglsbm29m3c/1633674675000/08765295497597902326/*/16cIRqrYt6DBBTPru2Dt6ghDpXq4ACZMa?e=download [following]\n","Warning: wildcards not supported in HTTP.\n","--2021-10-08 06:31:35--  https://doc-0k-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tgs86na8omrrcpv93tc86kglsbm29m3c/1633674675000/08765295497597902326/*/16cIRqrYt6DBBTPru2Dt6ghDpXq4ACZMa?e=download\n","Resolving doc-0k-a8-docs.googleusercontent.com (doc-0k-a8-docs.googleusercontent.com)... 209.85.200.132, 2607:f8b0:4001:c16::84\n","Connecting to doc-0k-a8-docs.googleusercontent.com (doc-0k-a8-docs.googleusercontent.com)|209.85.200.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4797 (4.7K) [text/plain]\n","Saving to: ‘CIFAR35200.py’\n","\n","CIFAR35200.py       100%[===================>]   4.68K  --.-KB/s    in 0s      \n","\n","2021-10-08 06:31:36 (11.1 MB/s) - ‘CIFAR35200.py’ saved [4797/4797]\n","\n","2021-10-08 06:31:38.947125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:38.961143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:38.961901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","INFO[build.py]: Loading dataset  from path: /content/drive/My Drive/Deep Learning System/assignment-2/data/CIFAR35200/CIFAR35200.py\n","2021-10-08 06:31:39.210372: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2021-10-08 06:31:39.270804: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","INFO[build.py]: download_and_prepare for dataset cifar35200/1.0.0...\n","INFO[dataset_builder.py]: Generating dataset cifar35200 (/root/tensorflow_datasets/cifar35200/1.0.0)\n","\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/cifar35200/1.0.0...\u001b[0m\n","2021-10-08 06:31:39.669695: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2021-10-08 06:31:39.709169: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","Generating splits...:   0% 0/3 [00:00<?, ? splits/s]\n","Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A2021-10-08 06:31:39.864070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:39.864968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:39.865742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:44.829391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:44.830302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:44.831067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-10-08 06:31:44.831762: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-10-08 06:31:44.831843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10256 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","\n","Generating train examples...: 1 examples [00:05,  5.13s/ examples]\u001b[A\n","Generating train examples...: 107 examples [00:05, 28.89 examples/s]\u001b[A\n","Generating train examples...: 206 examples [00:05, 64.68 examples/s]\u001b[A\n","Generating train examples...: 313 examples [00:05, 115.42 examples/s]\u001b[A\n","Generating train examples...: 417 examples [00:05, 177.61 examples/s]\u001b[A\n","Generating train examples...: 516 examples [00:05, 248.78 examples/s]\u001b[A\n","Generating train examples...: 613 examples [00:05, 327.35 examples/s]\u001b[A\n","Generating train examples...: 709 examples [00:05, 413.37 examples/s]\u001b[A\n","Generating train examples...: 805 examples [00:05, 501.24 examples/s]\u001b[A\n","Generating train examples...: 900 examples [00:06, 584.65 examples/s]\u001b[A\n","Generating train examples...: 1000 examples [00:06, 671.93 examples/s]\u001b[A\n","Generating train examples...: 1109 examples [00:06, 768.22 examples/s]\u001b[A\n","Generating train examples...: 1209 examples [00:06, 821.19 examples/s]\u001b[A\n","Generating train examples...: 1314 examples [00:06, 878.74 examples/s]\u001b[A\n","Generating train examples...: 1415 examples [00:06, 902.91 examples/s]\u001b[A\n","Generating train examples...: 1516 examples [00:06, 931.23 examples/s]\u001b[A\n","Generating train examples...: 1616 examples [00:06, 950.65 examples/s]\u001b[A\n","Generating train examples...: 1716 examples [00:06, 956.45 examples/s]\u001b[A\n","Generating train examples...: 1815 examples [00:06, 952.31 examples/s]\u001b[A\n","Generating train examples...: 1914 examples [00:07, 961.38 examples/s]\u001b[A\n","Generating train examples...: 2014 examples [00:07, 971.74 examples/s]\u001b[A\n","Generating train examples...: 2121 examples [00:07, 998.92 examples/s]\u001b[A\n","Generating train examples...: 2223 examples [00:07, 1003.08 examples/s]\u001b[A\n","Generating train examples...: 2324 examples [00:07, 1000.64 examples/s]\u001b[A\n","Generating train examples...: 2425 examples [00:07, 998.75 examples/s] \u001b[A\n","Generating train examples...: 2526 examples [00:07, 992.25 examples/s]\u001b[A\n","Generating train examples...: 2626 examples [00:07, 991.53 examples/s]\u001b[A\n","Generating train examples...: 2726 examples [00:07, 986.92 examples/s]\u001b[A\n","Generating train examples...: 2825 examples [00:07, 986.23 examples/s]\u001b[A\n","Generating train examples...: 2929 examples [00:08, 999.75 examples/s]\u001b[A\n","Generating train examples...: 3030 examples [00:08, 989.92 examples/s]\u001b[A\n","Generating train examples...: 3130 examples [00:08, 972.59 examples/s]\u001b[A\n","Generating train examples...: 3228 examples [00:08, 931.78 examples/s]\u001b[A\n","Generating train examples...: 3322 examples [00:08, 930.22 examples/s]\u001b[A\n","Generating train examples...: 3425 examples [00:08, 957.33 examples/s]\u001b[A\n","Generating train examples...: 3522 examples [00:08, 960.68 examples/s]\u001b[A\n","Generating train examples...: 3624 examples [00:08, 976.76 examples/s]\u001b[A\n","Generating train examples...: 3722 examples [00:08, 972.25 examples/s]\u001b[A\n","Generating train examples...: 3820 examples [00:09, 967.25 examples/s]\u001b[A\n","Generating train examples...: 3924 examples [00:09, 987.73 examples/s]\u001b[A\n","Generating train examples...: 4023 examples [00:09, 987.83 examples/s]\u001b[A\n","Generating train examples...: 4130 examples [00:09, 1010.78 examples/s]\u001b[A\n","Generating train examples...: 4232 examples [00:09, 977.25 examples/s] \u001b[A\n","Generating train examples...: 4335 examples [00:09, 990.83 examples/s]\u001b[A\n","Generating train examples...: 4435 examples [00:09, 977.08 examples/s]\u001b[A\n","Generating train examples...: 4541 examples [00:09, 1000.49 examples/s]\u001b[A\n","Generating train examples...: 4642 examples [00:09, 980.83 examples/s] \u001b[A\n","Generating train examples...: 4741 examples [00:09, 977.55 examples/s]\u001b[A\n","Generating train examples...: 4840 examples [00:10, 979.35 examples/s]\u001b[A\n","Generating train examples...: 4945 examples [00:10, 998.12 examples/s]\u001b[A\n","Generating train examples...: 5046 examples [00:10, 999.24 examples/s]\u001b[A\n","Generating train examples...: 5146 examples [00:10, 998.70 examples/s]\u001b[A\n","Generating train examples...: 5248 examples [00:10, 995.46 examples/s]\u001b[A\n","Generating train examples...: 5350 examples [00:10, 1000.53 examples/s]\u001b[A\n","Generating train examples...: 5451 examples [00:10, 999.67 examples/s] \u001b[A\n","Generating train examples...: 5557 examples [00:10, 1015.81 examples/s]\u001b[A\n","Generating train examples...: 5659 examples [00:10, 997.97 examples/s] \u001b[A\n","Generating train examples...: 5759 examples [00:10, 984.18 examples/s]\u001b[A\n","Generating train examples...: 5860 examples [00:11, 991.71 examples/s]\u001b[A\n","Generating train examples...: 5960 examples [00:11, 985.38 examples/s]\u001b[A\n","Generating train examples...: 6059 examples [00:11, 972.27 examples/s]\u001b[A\n","Generating train examples...: 6163 examples [00:11, 990.65 examples/s]\u001b[A\n","Generating train examples...: 6263 examples [00:11, 984.94 examples/s]\u001b[A\n","Generating train examples...: 6368 examples [00:11, 1002.62 examples/s]\u001b[A\n","Generating train examples...: 6469 examples [00:11, 974.37 examples/s] \u001b[A\n","Generating train examples...: 6576 examples [00:11, 1001.55 examples/s]\u001b[A\n","Generating train examples...: 6677 examples [00:11, 947.49 examples/s] \u001b[A\n","Generating train examples...: 6773 examples [00:12, 924.12 examples/s]\u001b[A\n","Generating train examples...: 6868 examples [00:12, 930.62 examples/s]\u001b[A\n","Generating train examples...: 6975 examples [00:12, 968.12 examples/s]\u001b[A\n","Generating train examples...: 7073 examples [00:12, 968.42 examples/s]\u001b[A\n","Generating train examples...: 7171 examples [00:12, 967.66 examples/s]\u001b[A\n","Generating train examples...: 7272 examples [00:12, 979.43 examples/s]\u001b[A\n","Generating train examples...: 7371 examples [00:12, 976.28 examples/s]\u001b[A\n","Generating train examples...: 7472 examples [00:12, 984.90 examples/s]\u001b[A\n","Generating train examples...: 7571 examples [00:12, 972.25 examples/s]\u001b[A\n","Generating train examples...: 7674 examples [00:12, 989.04 examples/s]\u001b[A\n","Generating train examples...: 7777 examples [00:13, 999.40 examples/s]\u001b[A\n","Generating train examples...: 7878 examples [00:13, 997.86 examples/s]\u001b[A\n","Generating train examples...: 7978 examples [00:13, 998.30 examples/s]\u001b[A\n","Generating train examples...: 8078 examples [00:13, 994.73 examples/s]\u001b[A\n","Generating train examples...: 8178 examples [00:13, 989.68 examples/s]\u001b[A\n","Generating train examples...: 8277 examples [00:13, 988.55 examples/s]\u001b[A\n","Generating train examples...: 8376 examples [00:13, 987.50 examples/s]\u001b[A\n","Generating train examples...: 8475 examples [00:13, 985.07 examples/s]\u001b[A\n","Generating train examples...: 8576 examples [00:13, 989.71 examples/s]\u001b[A\n","Generating train examples...: 8676 examples [00:13, 991.58 examples/s]\u001b[A\n","Generating train examples...: 8778 examples [00:14, 998.49 examples/s]\u001b[A\n","Generating train examples...: 8885 examples [00:14, 1017.42 examples/s]\u001b[A\n","Generating train examples...: 8987 examples [00:14, 1014.44 examples/s]\u001b[A\n","Generating train examples...: 9091 examples [00:14, 1019.74 examples/s]\u001b[A\n","Generating train examples...: 9197 examples [00:14, 1028.97 examples/s]\u001b[A\n","Generating train examples...: 9300 examples [00:14, 1022.64 examples/s]\u001b[A\n","Generating train examples...: 9403 examples [00:14, 1016.60 examples/s]\u001b[A\n","Generating train examples...: 9505 examples [00:14, 1008.11 examples/s]\u001b[A\n","Generating train examples...: 9606 examples [00:14, 995.58 examples/s] \u001b[A\n","Generating train examples...: 9710 examples [00:14, 1007.14 examples/s]\u001b[A\n","Generating train examples...: 9811 examples [00:15, 1006.94 examples/s]\u001b[A\n","Generating train examples...: 9913 examples [00:15, 1007.87 examples/s]\u001b[A\n","Generating train examples...: 10014 examples [00:15, 754.67 examples/s]\u001b[A\n","Generating train examples...: 10114 examples [00:15, 813.51 examples/s]\u001b[A\n","Generating train examples...: 10215 examples [00:15, 862.85 examples/s]\u001b[A\n","Generating train examples...: 10315 examples [00:15, 899.53 examples/s]\u001b[A\n","Generating train examples...: 10416 examples [00:15, 928.73 examples/s]\u001b[A\n","Generating train examples...: 10515 examples [00:15, 944.12 examples/s]\u001b[A\n","Generating train examples...: 10613 examples [00:15, 942.37 examples/s]\u001b[A\n","Generating train examples...: 10716 examples [00:16, 965.69 examples/s]\u001b[A\n","Generating train examples...: 10814 examples [00:16, 957.12 examples/s]\u001b[A\n","Generating train examples...: 10912 examples [00:16, 962.75 examples/s]\u001b[A\n","Generating train examples...: 11011 examples [00:16, 969.65 examples/s]\u001b[A\n","Generating train examples...: 11111 examples [00:16, 978.33 examples/s]\u001b[A\n","Generating train examples...: 11219 examples [00:16, 1007.45 examples/s]\u001b[A\n","Generating train examples...: 11321 examples [00:16, 997.31 examples/s] \u001b[A\n","Generating train examples...: 11428 examples [00:16, 1018.74 examples/s]\u001b[A\n","Generating train examples...: 11531 examples [00:16, 965.51 examples/s] \u001b[A\n","Generating train examples...: 11631 examples [00:17, 973.17 examples/s]\u001b[A\n","Generating train examples...: 11734 examples [00:17, 989.52 examples/s]\u001b[A\n","Generating train examples...: 11834 examples [00:17, 988.79 examples/s]\u001b[A\n","Generating train examples...: 11936 examples [00:17, 997.40 examples/s]\u001b[A\n","Generating train examples...: 12038 examples [00:17, 1001.50 examples/s]\u001b[A\n","Generating train examples...: 12145 examples [00:17, 1019.63 examples/s]\u001b[A\n","Generating train examples...: 12250 examples [00:17, 1027.33 examples/s]\u001b[A\n","Generating train examples...: 12353 examples [00:17, 1015.19 examples/s]\u001b[A\n","Generating train examples...: 12459 examples [00:17, 1026.55 examples/s]\u001b[A\n","Generating train examples...: 12562 examples [00:17, 992.79 examples/s] \u001b[A\n","Generating train examples...: 12665 examples [00:18, 1002.86 examples/s]\u001b[A\n","Generating train examples...: 12766 examples [00:18, 982.84 examples/s] \u001b[A\n","Generating train examples...: 12866 examples [00:18, 985.25 examples/s]\u001b[A\n","Generating train examples...: 12973 examples [00:18, 1009.44 examples/s]\u001b[A\n","Generating train examples...: 13075 examples [00:18, 1001.48 examples/s]\u001b[A\n","Generating train examples...: 13176 examples [00:18, 998.28 examples/s] \u001b[A\n","Generating train examples...: 13276 examples [00:18, 985.53 examples/s]\u001b[A\n","Generating train examples...: 13378 examples [00:18, 995.19 examples/s]\u001b[A\n","Generating train examples...: 13484 examples [00:18, 1012.99 examples/s]\u001b[A\n","Generating train examples...: 13586 examples [00:18, 983.34 examples/s] \u001b[A\n","Generating train examples...: 13690 examples [00:19, 999.69 examples/s]\u001b[A\n","Generating train examples...: 13791 examples [00:19, 986.07 examples/s]\u001b[A\n","Generating train examples...: 13897 examples [00:19, 1005.22 examples/s]\u001b[A\n","Generating train examples...: 13999 examples [00:19, 1006.94 examples/s]\u001b[A\n","Generating train examples...: 14103 examples [00:19, 1015.07 examples/s]\u001b[A\n","Generating train examples...: 14205 examples [00:19, 991.48 examples/s] \u001b[A\n","Generating train examples...: 14311 examples [00:19, 1008.24 examples/s]\u001b[A\n","Generating train examples...: 14418 examples [00:19, 1024.27 examples/s]\u001b[A\n","Generating train examples...: 14523 examples [00:19, 1029.46 examples/s]\u001b[A\n","Generating train examples...: 14627 examples [00:19, 969.74 examples/s] \u001b[A\n","Generating train examples...: 14725 examples [00:20, 969.88 examples/s]\u001b[A\n","Generating train examples...: 14823 examples [00:20, 953.27 examples/s]\u001b[A\n","Generating train examples...: 14924 examples [00:20, 968.04 examples/s]\u001b[A\n","Generating train examples...: 15022 examples [00:20, 967.98 examples/s]\u001b[A\n","Generating train examples...: 15122 examples [00:20, 974.95 examples/s]\u001b[A\n","Generating train examples...: 15220 examples [00:20, 974.91 examples/s]\u001b[A\n","Generating train examples...: 15318 examples [00:20, 964.84 examples/s]\u001b[A\n","Generating train examples...: 15417 examples [00:20, 969.46 examples/s]\u001b[A\n","Generating train examples...: 15520 examples [00:20, 987.31 examples/s]\u001b[A\n","Generating train examples...: 15619 examples [00:21, 976.39 examples/s]\u001b[A\n","Generating train examples...: 15721 examples [00:21, 986.59 examples/s]\u001b[A\n","Generating train examples...: 15821 examples [00:21, 988.00 examples/s]\u001b[A\n","Generating train examples...: 15924 examples [00:21, 997.97 examples/s]\u001b[A\n","Generating train examples...: 16024 examples [00:21, 990.07 examples/s]\u001b[A\n","Generating train examples...: 16124 examples [00:21, 990.56 examples/s]\u001b[A\n","Generating train examples...: 16224 examples [00:21, 979.01 examples/s]\u001b[A\n","Generating train examples...: 16323 examples [00:21, 980.73 examples/s]\u001b[A\n","Generating train examples...: 16422 examples [00:21, 979.89 examples/s]\u001b[A\n","Generating train examples...: 16521 examples [00:21, 972.10 examples/s]\u001b[A\n","Generating train examples...: 16619 examples [00:22, 968.55 examples/s]\u001b[A\n","Generating train examples...: 16721 examples [00:22, 981.24 examples/s]\u001b[A\n","Generating train examples...: 16825 examples [00:22, 997.63 examples/s]\u001b[A\n","Generating train examples...: 16925 examples [00:22, 995.37 examples/s]\u001b[A\n","Generating train examples...: 17027 examples [00:22, 1000.12 examples/s]\u001b[A\n","Generating train examples...: 17128 examples [00:22, 1001.85 examples/s]\u001b[A\n","Generating train examples...: 17229 examples [00:22, 1003.62 examples/s]\u001b[A\n","Generating train examples...: 17330 examples [00:22, 1002.10 examples/s]\u001b[A\n","Generating train examples...: 17432 examples [00:22, 1006.41 examples/s]\u001b[A\n","Generating train examples...: 17533 examples [00:22, 1003.67 examples/s]\u001b[A\n","Generating train examples...: 17635 examples [00:23, 1006.84 examples/s]\u001b[A\n","Generating train examples...: 17736 examples [00:23, 1005.52 examples/s]\u001b[A\n","Generating train examples...: 17837 examples [00:23, 1001.32 examples/s]\u001b[A\n","Generating train examples...: 17938 examples [00:23, 984.33 examples/s] \u001b[A\n","Generating train examples...: 18037 examples [00:23, 981.45 examples/s]\u001b[A\n","Generating train examples...: 18142 examples [00:23, 999.99 examples/s]\u001b[A\n","Generating train examples...: 18243 examples [00:23, 990.26 examples/s]\u001b[A\n","Generating train examples...: 18343 examples [00:23, 987.95 examples/s]\u001b[A\n","Generating train examples...: 18442 examples [00:23, 986.21 examples/s]\u001b[A\n","Generating train examples...: 18541 examples [00:23, 985.19 examples/s]\u001b[A\n","Generating train examples...: 18640 examples [00:24, 975.25 examples/s]\u001b[A\n","Generating train examples...: 18741 examples [00:24, 985.11 examples/s]\u001b[A\n","Generating train examples...: 18848 examples [00:24, 1008.34 examples/s]\u001b[A\n","Generating train examples...: 18949 examples [00:24, 1002.08 examples/s]\u001b[A\n","Generating train examples...: 19054 examples [00:24, 1015.42 examples/s]\u001b[A\n","Generating train examples...: 19156 examples [00:24, 1015.29 examples/s]\u001b[A\n","Generating train examples...: 19258 examples [00:24, 1006.36 examples/s]\u001b[A\n","Generating train examples...: 19360 examples [00:24, 1009.08 examples/s]\u001b[A\n","Generating train examples...: 19464 examples [00:24, 1016.81 examples/s]\u001b[A\n","Generating train examples...: 19566 examples [00:24, 1006.26 examples/s]\u001b[A\n","Generating train examples...: 19667 examples [00:25, 1001.18 examples/s]\u001b[A\n","Generating train examples...: 19772 examples [00:25, 1013.90 examples/s]\u001b[A\n","Generating train examples...: 19875 examples [00:25, 1018.06 examples/s]\u001b[A\n","Generating train examples...: 19977 examples [00:25, 1001.75 examples/s]\u001b[A\n","Generating train examples...: 20078 examples [00:25, 792.65 examples/s] \u001b[A\n","Generating train examples...: 20183 examples [00:25, 856.04 examples/s]\u001b[A\n","Generating train examples...: 20289 examples [00:25, 909.56 examples/s]\u001b[A\n","Generating train examples...: 20393 examples [00:25, 945.00 examples/s]\u001b[A\n","Generating train examples...: 20493 examples [00:25, 959.05 examples/s]\u001b[A\n","Generating train examples...: 20592 examples [00:26, 954.38 examples/s]\u001b[A\n","Generating train examples...: 20697 examples [00:26, 978.90 examples/s]\u001b[A\n","Generating train examples...: 20797 examples [00:26, 953.96 examples/s]\u001b[A\n","Generating train examples...: 20898 examples [00:26, 967.88 examples/s]\u001b[A\n","Generating train examples...: 21000 examples [00:26, 981.15 examples/s]\u001b[A\n","Generating train examples...: 21102 examples [00:26, 990.30 examples/s]\u001b[A\n","Generating train examples...: 21202 examples [00:26, 986.40 examples/s]\u001b[A\n","Generating train examples...: 21304 examples [00:26, 995.29 examples/s]\u001b[A\n","Generating train examples...: 21404 examples [00:26, 984.57 examples/s]\u001b[A\n","Generating train examples...: 21507 examples [00:27, 996.66 examples/s]\u001b[A\n","Generating train examples...: 21607 examples [00:27, 961.79 examples/s]\u001b[A\n","Generating train examples...: 21710 examples [00:27, 978.92 examples/s]\u001b[A\n","Generating train examples...: 21816 examples [00:27, 1002.05 examples/s]\u001b[A\n","Generating train examples...: 21918 examples [00:27, 1006.51 examples/s]\u001b[A\n","Generating train examples...: 22025 examples [00:27, 1023.55 examples/s]\u001b[A\n","Generating train examples...: 22128 examples [00:27, 994.02 examples/s] \u001b[A\n","Generating train examples...: 22233 examples [00:27, 1010.18 examples/s]\u001b[A\n","Generating train examples...: 22335 examples [00:27, 1012.29 examples/s]\u001b[A\n","Generating train examples...: 22437 examples [00:27, 1012.85 examples/s]\u001b[A\n","Generating train examples...: 22539 examples [00:28, 976.52 examples/s] \u001b[A\n","Generating train examples...: 22637 examples [00:28, 932.68 examples/s]\u001b[A\n","Generating train examples...: 22735 examples [00:28, 944.50 examples/s]\u001b[A\n","Generating train examples...: 22839 examples [00:28, 970.45 examples/s]\u001b[A\n","Generating train examples...: 22948 examples [00:28, 1003.16 examples/s]\u001b[A\n","Generating train examples...: 23049 examples [00:28, 993.33 examples/s] \u001b[A\n","Generating train examples...: 23152 examples [00:28, 1002.67 examples/s]\u001b[A\n","Generating train examples...: 23253 examples [00:28, 1004.26 examples/s]\u001b[A\n","Generating train examples...: 23357 examples [00:28, 1012.56 examples/s]\u001b[A\n","Generating train examples...: 23459 examples [00:28, 985.84 examples/s] \u001b[A\n","Generating train examples...: 23558 examples [00:29, 980.26 examples/s]\u001b[A\n","Generating train examples...: 23666 examples [00:29, 1008.09 examples/s]\u001b[A\n","Generating train examples...: 23767 examples [00:29, 988.16 examples/s] \u001b[A\n","Generating train examples...: 23866 examples [00:29, 988.45 examples/s]\u001b[A\n","Generating train examples...: 23965 examples [00:29, 980.36 examples/s]\u001b[A\n","Generating train examples...: 24068 examples [00:29, 993.00 examples/s]\u001b[A\n","Generating train examples...: 24169 examples [00:29, 997.84 examples/s]\u001b[A\n","Generating train examples...: 24277 examples [00:29, 1020.53 examples/s]\u001b[A\n","Generating train examples...: 24380 examples [00:29, 1020.09 examples/s]\u001b[A\n","Generating train examples...: 24483 examples [00:29, 1010.99 examples/s]\u001b[A\n","Generating train examples...: 24585 examples [00:30, 985.52 examples/s] \u001b[A\n","Generating train examples...: 24684 examples [00:30, 980.53 examples/s]\u001b[A\n","Generating train examples...: 24787 examples [00:30, 993.25 examples/s]\u001b[A\n","Generating train examples...: 24893 examples [00:30, 992.69 examples/s]\u001b[A\n","Generating train examples...: 24997 examples [00:30, 1003.83 examples/s]\u001b[A\n","Generating train examples...: 25098 examples [00:30, 998.73 examples/s] \u001b[A\n","Generating train examples...: 25198 examples [00:30, 998.37 examples/s]\u001b[A\n","Generating train examples...: 25305 examples [00:30, 1017.08 examples/s]\u001b[A\n","Generating train examples...: 25407 examples [00:30, 992.00 examples/s] \u001b[A\n","Generating train examples...: 25507 examples [00:31, 991.14 examples/s]\u001b[A\n","Generating train examples...: 25607 examples [00:31, 965.72 examples/s]\u001b[A\n","Generating train examples...: 25704 examples [00:31, 966.58 examples/s]\u001b[A\n","Generating train examples...: 25809 examples [00:31, 988.74 examples/s]\u001b[A\n","Generating train examples...: 25911 examples [00:31, 997.91 examples/s]\u001b[A\n","Generating train examples...: 26011 examples [00:31, 997.19 examples/s]\u001b[A\n","Generating train examples...: 26111 examples [00:31, 992.85 examples/s]\u001b[A\n","Generating train examples...: 26212 examples [00:31, 995.37 examples/s]\u001b[A\n","Generating train examples...: 26312 examples [00:31, 994.21 examples/s]\u001b[A\n","Generating train examples...: 26412 examples [00:31, 961.64 examples/s]\u001b[A\n","Generating train examples...: 26519 examples [00:32, 991.96 examples/s]\u001b[A\n","Generating train examples...: 26619 examples [00:32, 971.54 examples/s]\u001b[A\n","Generating train examples...: 26724 examples [00:32, 993.06 examples/s]\u001b[A\n","Generating train examples...: 26824 examples [00:32, 972.13 examples/s]\u001b[A\n","Generating train examples...: 26922 examples [00:32, 922.23 examples/s]\u001b[A\n","Generating train examples...: 27025 examples [00:32, 950.19 examples/s]\u001b[A\n","Generating train examples...: 27121 examples [00:32, 943.50 examples/s]\u001b[A\n","Generating train examples...: 27229 examples [00:32, 982.33 examples/s]\u001b[A\n","Generating train examples...: 27328 examples [00:32, 978.08 examples/s]\u001b[A\n","Generating train examples...: 27427 examples [00:33, 972.44 examples/s]\u001b[A\n","Generating train examples...: 27528 examples [00:33, 982.51 examples/s]\u001b[A\n","Generating train examples...: 27627 examples [00:33, 966.79 examples/s]\u001b[A\n","Generating train examples...: 27724 examples [00:33, 963.93 examples/s]\u001b[A\n","Generating train examples...: 27822 examples [00:33, 967.38 examples/s]\u001b[A\n","Generating train examples...: 27924 examples [00:33, 980.48 examples/s]\u001b[A\n","Generating train examples...: 28030 examples [00:33, 1002.45 examples/s]\u001b[A\n","Generating train examples...: 28131 examples [00:33, 980.23 examples/s] \u001b[A\n","Generating train examples...: 28230 examples [00:33, 979.76 examples/s]\u001b[A\n","Generating train examples...: 28331 examples [00:33, 986.82 examples/s]\u001b[A\n","Generating train examples...: 28434 examples [00:34, 996.31 examples/s]\u001b[A\n","Generating train examples...: 28536 examples [00:34, 1000.26 examples/s]\u001b[A\n","Generating train examples...: 28637 examples [00:34, 981.18 examples/s] \u001b[A\n","Generating train examples...: 28743 examples [00:34, 1001.60 examples/s]\u001b[A\n","Generating train examples...: 28854 examples [00:34, 1031.04 examples/s]\u001b[A\n","Generating train examples...: 28958 examples [00:34, 1015.08 examples/s]\u001b[A\n","Generating train examples...: 29066 examples [00:34, 1031.24 examples/s]\u001b[A\n","Generating train examples...: 29170 examples [00:34, 1033.40 examples/s]\u001b[A\n","Generating train examples...: 29276 examples [00:34, 1040.92 examples/s]\u001b[A\n","Generating train examples...: 29381 examples [00:34, 1040.86 examples/s]\u001b[A\n","Generating train examples...: 29489 examples [00:35, 1050.40 examples/s]\u001b[A\n","Generating train examples...: 29595 examples [00:35, 1043.37 examples/s]\u001b[A\n","Generating train examples...: 29700 examples [00:35, 1027.02 examples/s]\u001b[A\n","Generating train examples...: 29809 examples [00:35, 1044.42 examples/s]\u001b[A\n","Generating train examples...: 29914 examples [00:35, 1041.50 examples/s]\u001b[A\n","Generating train examples...: 30019 examples [00:35, 841.32 examples/s] \u001b[A\n","Generating train examples...: 30120 examples [00:35, 882.93 examples/s]\u001b[A\n","Generating train examples...: 30221 examples [00:35, 916.46 examples/s]\u001b[A\n","Generating train examples...: 30326 examples [00:35, 951.92 examples/s]\u001b[A\n","Generating train examples...: 30426 examples [00:36, 964.23 examples/s]\u001b[A\n","Generating train examples...: 30533 examples [00:36, 994.00 examples/s]\u001b[A\n","Generating train examples...: 30635 examples [00:36, 983.21 examples/s]\u001b[A\n","Generating train examples...: 30743 examples [00:36, 1009.41 examples/s]\u001b[A\n","Generating train examples...: 30845 examples [00:36, 978.26 examples/s] \u001b[A\n","Generating train examples...: 30949 examples [00:36, 993.61 examples/s]\u001b[A\n","Generating train examples...: 31049 examples [00:36, 986.38 examples/s]\u001b[A\n","Generating train examples...: 31150 examples [00:36, 993.10 examples/s]\u001b[A\n","Generating train examples...: 31255 examples [00:36, 1007.19 examples/s]\u001b[A\n","Generating train examples...: 31356 examples [00:36, 1006.73 examples/s]\u001b[A\n","Generating train examples...: 31461 examples [00:37, 1019.17 examples/s]\u001b[A\n","Generating train examples...: 31564 examples [00:37, 993.75 examples/s] \u001b[A\n","Generating train examples...: 31669 examples [00:37, 1009.61 examples/s]\u001b[A\n","Generating train examples...: 31776 examples [00:37, 1026.63 examples/s]\u001b[A\n","Generating train examples...: 31879 examples [00:37, 1021.98 examples/s]\u001b[A\n","Generating train examples...: 31982 examples [00:37, 1017.10 examples/s]\u001b[A\n","Generating train examples...: 32084 examples [00:37, 982.45 examples/s] \u001b[A\n","Generating train examples...: 32186 examples [00:37, 992.97 examples/s]\u001b[A\n","Generating train examples...: 32286 examples [00:37, 988.13 examples/s]\u001b[A\n","Generating train examples...: 32385 examples [00:37, 986.43 examples/s]\u001b[A\n","Generating train examples...: 32486 examples [00:38, 991.28 examples/s]\u001b[A\n","Generating train examples...: 32586 examples [00:38, 977.44 examples/s]\u001b[A\n","Generating train examples...: 32688 examples [00:38, 987.85 examples/s]\u001b[A\n","Generating train examples...: 32787 examples [00:38, 978.28 examples/s]\u001b[A\n","Generating train examples...: 32887 examples [00:38, 982.54 examples/s]\u001b[A\n","Generating train examples...: 32997 examples [00:38, 1014.68 examples/s]\u001b[A\n","Generating train examples...: 33099 examples [00:38, 998.30 examples/s] \u001b[A\n","Generating train examples...: 33206 examples [00:38, 1017.25 examples/s]\u001b[A\n","Generating train examples...: 33309 examples [00:38, 1020.78 examples/s]\u001b[A\n","Generating train examples...: 33414 examples [00:39, 1023.94 examples/s]\u001b[A\n","Generating train examples...: 33517 examples [00:39, 1014.51 examples/s]\u001b[A\n","Generating train examples...: 33619 examples [00:39, 993.28 examples/s] \u001b[A\n","Generating train examples...: 33719 examples [00:39, 964.90 examples/s]\u001b[A\n","Generating train examples...: 33817 examples [00:39, 966.90 examples/s]\u001b[A\n","Generating train examples...: 33919 examples [00:39, 981.14 examples/s]\u001b[A\n","Generating train examples...: 34018 examples [00:39, 967.92 examples/s]\u001b[A\n","Generating train examples...: 34120 examples [00:39, 981.42 examples/s]\u001b[A\n","Generating train examples...: 34219 examples [00:39, 981.91 examples/s]\u001b[A\n","Generating train examples...: 34322 examples [00:39, 995.78 examples/s]\u001b[A\n","Generating train examples...: 34423 examples [00:40, 999.88 examples/s]\u001b[A\n","Generating train examples...: 34526 examples [00:40, 1007.32 examples/s]\u001b[A\n","Generating train examples...: 34627 examples [00:40, 997.08 examples/s] \u001b[A\n","Generating train examples...: 34728 examples [00:40, 998.64 examples/s]\u001b[A\n","Generating train examples...: 34832 examples [00:40, 1010.35 examples/s]\u001b[A\n","Generating train examples...: 34938 examples [00:40, 1022.98 examples/s]\u001b[A\n","Generating train examples...: 35041 examples [00:40, 1012.41 examples/s]\u001b[A\n","Generating train examples...: 35143 examples [00:40, 1007.89 examples/s]\u001b[A\n","Generating train examples...: 35245 examples [00:40, 1009.01 examples/s]\u001b[A\n","Generating train examples...: 35346 examples [00:40, 991.37 examples/s] \u001b[A\n","Generating train examples...: 35446 examples [00:41, 991.38 examples/s]\u001b[A\n","Generating train examples...: 35549 examples [00:41, 1001.09 examples/s]\u001b[A\n","Generating train examples...: 35650 examples [00:41, 982.63 examples/s] \u001b[A\n","Generating train examples...: 35750 examples [00:41, 985.16 examples/s]\u001b[A\n","Generating train examples...: 35849 examples [00:41, 982.00 examples/s]\u001b[A\n","Generating train examples...: 35952 examples [00:41, 996.15 examples/s]\u001b[A\n","Generating train examples...: 36052 examples [00:41, 985.50 examples/s]\u001b[A\n","Generating train examples...: 36155 examples [00:41, 997.37 examples/s]\u001b[A\n","Generating train examples...: 36260 examples [00:41, 1012.78 examples/s]\u001b[A\n","Generating train examples...: 36362 examples [00:41, 1005.41 examples/s]\u001b[A\n","Generating train examples...: 36465 examples [00:42, 1010.20 examples/s]\u001b[A\n","Generating train examples...: 36567 examples [00:42, 989.05 examples/s] \u001b[A\n","Generating train examples...: 36667 examples [00:42, 964.27 examples/s]\u001b[A\n","Generating train examples...: 36769 examples [00:42, 979.60 examples/s]\u001b[A\n","Generating train examples...: 36868 examples [00:42, 971.73 examples/s]\u001b[A\n","Generating train examples...: 36966 examples [00:42, 933.36 examples/s]\u001b[A\n","Generating train examples...: 37066 examples [00:42, 950.64 examples/s]\u001b[A\n","Generating train examples...: 37163 examples [00:42, 954.57 examples/s]\u001b[A\n","Generating train examples...: 37266 examples [00:42, 975.08 examples/s]\u001b[A\n","Generating train examples...: 37364 examples [00:43, 963.93 examples/s]\u001b[A\n","Generating train examples...: 37462 examples [00:43, 966.26 examples/s]\u001b[A\n","Generating train examples...: 37562 examples [00:43, 973.62 examples/s]\u001b[A\n","Generating train examples...: 37660 examples [00:43, 956.57 examples/s]\u001b[A\n","Generating train examples...: 37759 examples [00:43, 964.97 examples/s]\u001b[A\n","Generating train examples...: 37856 examples [00:43, 960.84 examples/s]\u001b[A\n","Generating train examples...: 37953 examples [00:43, 952.11 examples/s]\u001b[A\n","Generating train examples...: 38054 examples [00:43, 967.25 examples/s]\u001b[A\n","Generating train examples...: 38156 examples [00:43, 981.12 examples/s]\u001b[A\n","Generating train examples...: 38260 examples [00:43, 997.34 examples/s]\u001b[A\n","Generating train examples...: 38360 examples [00:44, 993.31 examples/s]\u001b[A\n","Generating train examples...: 38464 examples [00:44, 1005.96 examples/s]\u001b[A\n","Generating train examples...: 38566 examples [00:44, 1007.74 examples/s]\u001b[A\n","Generating train examples...: 38667 examples [00:44, 995.98 examples/s] \u001b[A\n","Generating train examples...: 38767 examples [00:44, 990.47 examples/s]\u001b[A\n","Generating train examples...: 38867 examples [00:44, 983.48 examples/s]\u001b[A\n","Generating train examples...: 38967 examples [00:44, 984.57 examples/s]\u001b[A\n","Generating train examples...: 39069 examples [00:44, 993.95 examples/s]\u001b[A\n","Generating train examples...: 39176 examples [00:44, 1015.55 examples/s]\u001b[A\n","Generating train examples...: 39278 examples [00:44, 993.14 examples/s] \u001b[A\n","Generating train examples...: 39379 examples [00:45, 997.02 examples/s]\u001b[A\n","Generating train examples...: 39480 examples [00:45, 999.33 examples/s]\u001b[A\n","Generating train examples...: 39581 examples [00:45, 997.82 examples/s]\u001b[A\n","Generating train examples...: 39681 examples [00:45, 984.82 examples/s]\u001b[A\n","Generating train examples...: 39786 examples [00:45, 1003.74 examples/s]\u001b[A\n","Generating train examples...: 39887 examples [00:45, 973.40 examples/s] \u001b[A\n","Generating train examples...: 39985 examples [00:45, 970.06 examples/s]\u001b[A\n","Generating train examples...: 40083 examples [00:45, 761.03 examples/s]\u001b[A\n","Generating train examples...: 40185 examples [00:45, 824.32 examples/s]\u001b[A\n","Generating train examples...: 40290 examples [00:46, 882.92 examples/s]\u001b[A\n","Generating train examples...: 40390 examples [00:46, 913.06 examples/s]\u001b[A\n","Generating train examples...: 40493 examples [00:46, 944.17 examples/s]\u001b[A\n","Generating train examples...: 40591 examples [00:46, 930.68 examples/s]\u001b[A\n","Generating train examples...: 40696 examples [00:46, 964.43 examples/s]\u001b[A\n","Generating train examples...: 40797 examples [00:46, 974.89 examples/s]\u001b[A\n","Generating train examples...: 40896 examples [00:46, 967.25 examples/s]\u001b[A\n","Generating train examples...: 40995 examples [00:46, 973.07 examples/s]\u001b[A\n","Generating train examples...: 41093 examples [00:46, 950.28 examples/s]\u001b[A\n","Generating train examples...: 41196 examples [00:47, 971.81 examples/s]\u001b[A\n","Generating train examples...: 41294 examples [00:47, 964.17 examples/s]\u001b[A\n","Generating train examples...: 41391 examples [00:47, 945.94 examples/s]\u001b[A\n","Generating train examples...: 41486 examples [00:47, 930.06 examples/s]\u001b[A\n","Generating train examples...: 41583 examples [00:47, 941.43 examples/s]\u001b[A\n","Generating train examples...: 41683 examples [00:47, 958.05 examples/s]\u001b[A\n","Generating train examples...: 41779 examples [00:47, 945.99 examples/s]\u001b[A\n","Generating train examples...: 41881 examples [00:47, 967.44 examples/s]\u001b[A\n","Generating train examples...: 41983 examples [00:47, 982.70 examples/s]\u001b[A\n","Generating train examples...: 42082 examples [00:47, 981.51 examples/s]\u001b[A\n","Generating train examples...: 42187 examples [00:48, 999.47 examples/s]\u001b[A\n","Generating train examples...: 42288 examples [00:48, 995.13 examples/s]\u001b[A\n","Generating train examples...: 42388 examples [00:48, 987.25 examples/s]\u001b[A\n","Generating train examples...: 42487 examples [00:48, 972.66 examples/s]\u001b[A\n","Generating train examples...: 42585 examples [00:48, 970.76 examples/s]\u001b[A\n","Generating train examples...: 42690 examples [00:48, 993.80 examples/s]\u001b[A\n","Generating train examples...: 42790 examples [00:48, 973.70 examples/s]\u001b[A\n","Generating train examples...: 42890 examples [00:48, 978.89 examples/s]\u001b[A\n","Generating train examples...: 42988 examples [00:48, 964.61 examples/s]\u001b[A\n","Generating train examples...: 43085 examples [00:48, 965.89 examples/s]\u001b[A\n","Generating train examples...: 43187 examples [00:49, 979.81 examples/s]\u001b[A\n","Generating train examples...: 43294 examples [00:49, 1006.27 examples/s]\u001b[A\n","Generating train examples...: 43398 examples [00:49, 1013.59 examples/s]\u001b[A\n","Generating train examples...: 43500 examples [00:49, 997.96 examples/s] \u001b[A\n","Generating train examples...: 43606 examples [00:49, 1015.25 examples/s]\u001b[A\n","Generating train examples...: 43712 examples [00:49, 1027.72 examples/s]\u001b[A\n","Generating train examples...: 43815 examples [00:49, 1004.63 examples/s]\u001b[A\n","Generating train examples...: 43919 examples [00:49, 1013.47 examples/s]\u001b[A\n","Generating train examples...: 44021 examples [00:49, 989.80 examples/s] \u001b[A\n","Generating train examples...: 44121 examples [00:49, 987.15 examples/s]\u001b[A\n","Generating train examples...: 44224 examples [00:50, 998.77 examples/s]\u001b[A\n","Generating train examples...: 44324 examples [00:50, 932.73 examples/s]\u001b[A\n","Generating train examples...: 44419 examples [00:50, 936.95 examples/s]\u001b[A\n","Generating train examples...: 44521 examples [00:50, 958.64 examples/s]\u001b[A\n","Generating train examples...: 44618 examples [00:50, 953.70 examples/s]\u001b[A\n","Generating train examples...: 44723 examples [00:50, 979.40 examples/s]\u001b[A\n","Generating train examples...: 44822 examples [00:50, 968.04 examples/s]\u001b[A\n","Generating train examples...: 44925 examples [00:50, 983.52 examples/s]\u001b[A\n","Generating train examples...: 45029 examples [00:50, 1000.03 examples/s]\u001b[A\n","Generating train examples...: 45132 examples [00:51, 1008.59 examples/s]\u001b[A\n","Generating train examples...: 45234 examples [00:51, 1011.44 examples/s]\u001b[A\n","Generating train examples...: 45343 examples [00:51, 1032.23 examples/s]\u001b[A\n","Generating train examples...: 45447 examples [00:51, 1012.92 examples/s]\u001b[A\n","Generating train examples...: 45549 examples [00:51, 1002.77 examples/s]\u001b[A\n","Generating train examples...: 45658 examples [00:51, 1027.60 examples/s]\u001b[A\n","Generating train examples...: 45762 examples [00:51, 1030.72 examples/s]\u001b[A\n","Generating train examples...: 45866 examples [00:51, 1006.36 examples/s]\u001b[A\n","Generating train examples...: 45970 examples [00:51, 1015.37 examples/s]\u001b[A\n","Generating train examples...: 46072 examples [00:51, 993.81 examples/s] \u001b[A\n","Generating train examples...: 46172 examples [00:52, 992.68 examples/s]\u001b[A\n","Generating train examples...: 46272 examples [00:52, 987.92 examples/s]\u001b[A\n","Generating train examples...: 46378 examples [00:52, 1008.37 examples/s]\u001b[A\n","Generating train examples...: 46485 examples [00:52, 1026.42 examples/s]\u001b[A\n","Generating train examples...: 46588 examples [00:52, 988.69 examples/s] \u001b[A\n","Generating train examples...: 46691 examples [00:52, 1000.20 examples/s]\u001b[A\n","Generating train examples...: 46792 examples [00:52, 942.71 examples/s] \u001b[A\n","Generating train examples...: 46899 examples [00:52, 978.21 examples/s]\u001b[A\n","Generating train examples...: 46998 examples [00:52, 961.93 examples/s]\u001b[A\n","Generating train examples...: 47100 examples [00:52, 974.28 examples/s]\u001b[A\n","Generating train examples...: 47205 examples [00:53, 995.60 examples/s]\u001b[A\n","Generating train examples...: 47306 examples [00:53, 997.37 examples/s]\u001b[A\n","Generating train examples...: 47414 examples [00:53, 1019.44 examples/s]\u001b[A\n","Generating train examples...: 47517 examples [00:53, 980.81 examples/s] \u001b[A\n","Generating train examples...: 47616 examples [00:53, 982.11 examples/s]\u001b[A\n","Generating train examples...: 47722 examples [00:53, 1002.94 examples/s]\u001b[A\n","Generating train examples...: 47823 examples [00:53, 994.08 examples/s] \u001b[A\n","Generating train examples...: 47927 examples [00:53, 1007.33 examples/s]\u001b[A\n","Generating train examples...: 48028 examples [00:53, 1000.81 examples/s]\u001b[A\n","Generating train examples...: 48129 examples [00:54, 1000.77 examples/s]\u001b[A\n","Generating train examples...: 48234 examples [00:54, 1015.12 examples/s]\u001b[A\n","Generating train examples...: 48336 examples [00:54, 999.97 examples/s] \u001b[A\n","Generating train examples...: 48439 examples [00:54, 1008.18 examples/s]\u001b[A\n","Generating train examples...: 48540 examples [00:54, 968.48 examples/s] \u001b[A\n","Generating train examples...: 48639 examples [00:54, 974.05 examples/s]\u001b[A\n","Generating train examples...: 48742 examples [00:54, 988.91 examples/s]\u001b[A\n","Generating train examples...: 48842 examples [00:54, 961.53 examples/s]\u001b[A\n","Generating train examples...: 48939 examples [00:54, 935.92 examples/s]\u001b[A\n","Generating train examples...: 49034 examples [00:54, 937.58 examples/s]\u001b[A\n","Generating train examples...: 49140 examples [00:55, 971.85 examples/s]\u001b[A\n","Generating train examples...: 49238 examples [00:55, 970.15 examples/s]\u001b[A\n","Generating train examples...: 49341 examples [00:55, 985.07 examples/s]\u001b[A\n","Generating train examples...: 49449 examples [00:55, 1010.80 examples/s]\u001b[A\n","Generating train examples...: 49551 examples [00:55, 980.68 examples/s] \u001b[A\n","Generating train examples...: 49653 examples [00:55, 989.35 examples/s]\u001b[A\n","Generating train examples...: 49755 examples [00:55, 998.16 examples/s]\u001b[A\n","Generating train examples...: 49855 examples [00:55, 993.23 examples/s]\u001b[A\n","Generating train examples...: 49955 examples [00:55, 985.69 examples/s]\u001b[A\n","                                                                       \u001b[A\n","Shuffling cifar35200-train.tfrecord...:   0% 0/50000 [00:00<?, ? examples/s]\u001b[A\n","Shuffling cifar35200-train.tfrecord...:  18% 9095/50000 [00:00<00:00, 90938.81 examples/s]\u001b[A\n","Shuffling cifar35200-train.tfrecord...:  51% 25267/50000 [00:00<00:00, 132564.96 examples/s]\u001b[A\n","Shuffling cifar35200-train.tfrecord...:  82% 40967/50000 [00:00<00:00, 143716.13 examples/s]\u001b[A\n","INFO[tfrecords_writer.py]: Done writing cifar35200-train.tfrecord. Number of examples: 50000 (shards: [50000])\n","Generating splits...:  33% 1/3 [00:56<01:52, 56.38s/ splits]\n","Generating valid examples...: 0 examples [00:00, ? examples/s]\u001b[A\n","Generating valid examples...: 7 examples [00:00, 69.59 examples/s]\u001b[A\n","Generating valid examples...: 102 examples [00:00, 583.95 examples/s]\u001b[A\n","Generating valid examples...: 205 examples [00:00, 786.39 examples/s]\u001b[A\n","Generating valid examples...: 305 examples [00:00, 868.08 examples/s]\u001b[A\n","Generating valid examples...: 396 examples [00:00, 882.77 examples/s]\u001b[A\n","Generating valid examples...: 495 examples [00:00, 916.67 examples/s]\u001b[A\n","Generating valid examples...: 588 examples [00:00, 919.03 examples/s]\u001b[A\n","Generating valid examples...: 688 examples [00:00, 941.99 examples/s]\u001b[A\n","Generating valid examples...: 783 examples [00:00, 913.54 examples/s]\u001b[A\n","Generating valid examples...: 875 examples [00:01, 912.67 examples/s]\u001b[A\n","Generating valid examples...: 967 examples [00:01, 906.22 examples/s]\u001b[A\n","Generating valid examples...: 1066 examples [00:01, 929.35 examples/s]\u001b[A\n","Generating valid examples...: 1170 examples [00:01, 959.94 examples/s]\u001b[A\n","Generating valid examples...: 1270 examples [00:01, 970.01 examples/s]\u001b[A\n","Generating valid examples...: 1368 examples [00:01, 971.58 examples/s]\u001b[A\n","Generating valid examples...: 1466 examples [00:01, 968.39 examples/s]\u001b[A\n","Generating valid examples...: 1568 examples [00:01, 983.28 examples/s]\u001b[A\n","Generating valid examples...: 1671 examples [00:01, 994.77 examples/s]\u001b[A\n","Generating valid examples...: 1771 examples [00:01, 991.62 examples/s]\u001b[A\n","Generating valid examples...: 1873 examples [00:02, 998.77 examples/s]\u001b[A\n","Generating valid examples...: 1973 examples [00:02, 953.06 examples/s]\u001b[A\n","Generating valid examples...: 2072 examples [00:02, 962.97 examples/s]\u001b[A\n","Generating valid examples...: 2174 examples [00:02, 978.60 examples/s]\u001b[A\n","Generating valid examples...: 2273 examples [00:02, 978.98 examples/s]\u001b[A\n","Generating valid examples...: 2372 examples [00:02, 952.12 examples/s]\u001b[A\n","Generating valid examples...: 2468 examples [00:02, 915.95 examples/s]\u001b[A\n","Generating valid examples...: 2570 examples [00:02, 945.21 examples/s]\u001b[A\n","Generating valid examples...: 2677 examples [00:02, 980.90 examples/s]\u001b[A\n","Generating valid examples...: 2776 examples [00:02, 982.19 examples/s]\u001b[A\n","Generating valid examples...: 2881 examples [00:03, 999.72 examples/s]\u001b[A\n","Generating valid examples...: 2982 examples [00:03, 969.65 examples/s]\u001b[A\n","Generating valid examples...: 3081 examples [00:03, 974.91 examples/s]\u001b[A\n","Generating valid examples...: 3188 examples [00:03, 1000.81 examples/s]\u001b[A\n","Generating valid examples...: 3289 examples [00:03, 997.85 examples/s] \u001b[A\n","Generating valid examples...: 3389 examples [00:03, 981.17 examples/s]\u001b[A\n","Generating valid examples...: 3488 examples [00:03, 979.04 examples/s]\u001b[A\n","Generating valid examples...: 3592 examples [00:03, 996.11 examples/s]\u001b[A\n","Generating valid examples...: 3693 examples [00:03, 997.41 examples/s]\u001b[A\n","Generating valid examples...: 3793 examples [00:03, 990.49 examples/s]\u001b[A\n","Generating valid examples...: 3896 examples [00:04, 1001.50 examples/s]\u001b[A\n","Generating valid examples...: 3997 examples [00:04, 938.66 examples/s] \u001b[A\n","Generating valid examples...: 4099 examples [00:04, 959.46 examples/s]\u001b[A\n","Generating valid examples...: 4202 examples [00:04, 977.47 examples/s]\u001b[A\n","Generating valid examples...: 4301 examples [00:04, 967.29 examples/s]\u001b[A\n","Generating valid examples...: 4403 examples [00:04, 981.65 examples/s]\u001b[A\n","Generating valid examples...: 4502 examples [00:04, 975.81 examples/s]\u001b[A\n","Generating valid examples...: 4605 examples [00:04, 990.50 examples/s]\u001b[A\n","Generating valid examples...: 4705 examples [00:04, 985.22 examples/s]\u001b[A\n","Generating valid examples...: 4804 examples [00:05, 949.65 examples/s]\u001b[A\n","Generating valid examples...: 4900 examples [00:05, 931.50 examples/s]\u001b[A\n","Generating valid examples...: 4995 examples [00:05, 935.58 examples/s]\u001b[A\n","Generating valid examples...: 5095 examples [00:05, 953.05 examples/s]\u001b[A\n","Generating valid examples...: 5202 examples [00:05, 987.10 examples/s]\u001b[A\n","Generating valid examples...: 5301 examples [00:05, 974.12 examples/s]\u001b[A\n","Generating valid examples...: 5399 examples [00:05, 975.35 examples/s]\u001b[A\n","Generating valid examples...: 5497 examples [00:05, 965.30 examples/s]\u001b[A\n","Generating valid examples...: 5597 examples [00:05, 974.83 examples/s]\u001b[A\n","Generating valid examples...: 5703 examples [00:05, 999.60 examples/s]\u001b[A\n","Generating valid examples...: 5804 examples [00:06, 992.07 examples/s]\u001b[A\n","Generating valid examples...: 5904 examples [00:06, 974.28 examples/s]\u001b[A\n","Generating valid examples...: 6009 examples [00:06, 996.29 examples/s]\u001b[A\n","Generating valid examples...: 6109 examples [00:06, 949.01 examples/s]\u001b[A\n","Generating valid examples...: 6212 examples [00:06, 971.24 examples/s]\u001b[A\n","Generating valid examples...: 6315 examples [00:06, 987.26 examples/s]\u001b[A\n","Generating valid examples...: 6415 examples [00:06, 969.14 examples/s]\u001b[A\n","Generating valid examples...: 6521 examples [00:06, 993.72 examples/s]\u001b[A\n","Generating valid examples...: 6622 examples [00:06, 997.04 examples/s]\u001b[A\n","Generating valid examples...: 6722 examples [00:07, 986.82 examples/s]\u001b[A\n","Generating valid examples...: 6821 examples [00:07, 982.73 examples/s]\u001b[A\n","Generating valid examples...: 6920 examples [00:07, 931.76 examples/s]\u001b[A\n","Generating valid examples...: 7018 examples [00:07, 942.75 examples/s]\u001b[A\n","Generating valid examples...: 7115 examples [00:07, 950.16 examples/s]\u001b[A\n","Generating valid examples...: 7215 examples [00:07, 962.26 examples/s]\u001b[A\n","Generating valid examples...: 7315 examples [00:07, 965.40 examples/s]\u001b[A\n","Generating valid examples...: 7420 examples [00:07, 989.05 examples/s]\u001b[A\n","Generating valid examples...: 7520 examples [00:07, 983.47 examples/s]\u001b[A\n","Generating valid examples...: 7622 examples [00:07, 993.97 examples/s]\u001b[A\n","Generating valid examples...: 7722 examples [00:08, 981.72 examples/s]\u001b[A\n","Generating valid examples...: 7824 examples [00:08, 991.68 examples/s]\u001b[A\n","Generating valid examples...: 7924 examples [00:08, 984.40 examples/s]\u001b[A\n","Generating valid examples...: 8023 examples [00:08, 975.59 examples/s]\u001b[A\n","Generating valid examples...: 8128 examples [00:08, 997.42 examples/s]\u001b[A\n","Generating valid examples...: 8230 examples [00:08, 1003.50 examples/s]\u001b[A\n","Generating valid examples...: 8333 examples [00:08, 1009.20 examples/s]\u001b[A\n","Generating valid examples...: 8438 examples [00:08, 1018.63 examples/s]\u001b[A\n","Generating valid examples...: 8540 examples [00:08, 997.35 examples/s] \u001b[A\n","Generating valid examples...: 8645 examples [00:08, 1011.69 examples/s]\u001b[A\n","Generating valid examples...: 8747 examples [00:09, 995.09 examples/s] \u001b[A\n","Generating valid examples...: 8848 examples [00:09, 999.31 examples/s]\u001b[A\n","Generating valid examples...: 8949 examples [00:09, 999.84 examples/s]\u001b[A\n","Generating valid examples...: 9050 examples [00:09, 986.86 examples/s]\u001b[A\n","Generating valid examples...: 9149 examples [00:09, 987.65 examples/s]\u001b[A\n","Generating valid examples...: 9250 examples [00:09, 993.27 examples/s]\u001b[A\n","Generating valid examples...: 9350 examples [00:09, 978.69 examples/s]\u001b[A\n","Generating valid examples...: 9456 examples [00:09, 1002.20 examples/s]\u001b[A\n","Generating valid examples...: 9557 examples [00:09, 986.30 examples/s] \u001b[A\n","Generating valid examples...: 9658 examples [00:09, 991.80 examples/s]\u001b[A\n","Generating valid examples...: 9758 examples [00:10, 989.95 examples/s]\u001b[A\n","Generating valid examples...: 9858 examples [00:10, 971.15 examples/s]\u001b[A\n","Generating valid examples...: 9956 examples [00:10, 961.47 examples/s]\u001b[A\n","                                                                      \u001b[A\n","Shuffling cifar35200-valid.tfrecord...:   0% 0/10000 [00:00<?, ? examples/s]\u001b[A\n","INFO[tfrecords_writer.py]: Done writing cifar35200-valid.tfrecord. Number of examples: 10000 (shards: [10000])\n","Generating splits...:  67% 2/3 [01:06<00:29, 29.36s/ splits]\n","Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n","Generating test examples...: 77 examples [00:00, 768.46 examples/s]\u001b[A\n","Generating test examples...: 177 examples [00:00, 903.87 examples/s]\u001b[A\n","Generating test examples...: 280 examples [00:00, 958.78 examples/s]\u001b[A\n","Generating test examples...: 387 examples [00:00, 1000.42 examples/s]\u001b[A\n","Generating test examples...: 488 examples [00:00, 997.15 examples/s] \u001b[A\n","Generating test examples...: 597 examples [00:00, 1028.29 examples/s]\u001b[A\n","Generating test examples...: 700 examples [00:00, 1012.97 examples/s]\u001b[A\n","Generating test examples...: 802 examples [00:00, 984.96 examples/s] \u001b[A\n","Generating test examples...: 910 examples [00:00, 1011.59 examples/s]\u001b[A\n","Generating test examples...: 1012 examples [00:01, 1006.58 examples/s]\u001b[A\n","Generating test examples...: 1113 examples [00:01, 1006.76 examples/s]\u001b[A\n","Generating test examples...: 1214 examples [00:01, 975.56 examples/s] \u001b[A\n","Generating test examples...: 1320 examples [00:01, 999.02 examples/s]\u001b[A\n","Generating test examples...: 1421 examples [00:01, 999.70 examples/s]\u001b[A\n","Generating test examples...: 1526 examples [00:01, 1012.08 examples/s]\u001b[A\n","Generating test examples...: 1628 examples [00:01, 979.17 examples/s] \u001b[A\n","Generating test examples...: 1729 examples [00:01, 985.90 examples/s]\u001b[A\n","Generating test examples...: 1828 examples [00:01, 959.53 examples/s]\u001b[A\n","Generating test examples...: 1927 examples [00:01, 966.29 examples/s]\u001b[A\n","                                                                     \u001b[A\n","Shuffling cifar35200-test.tfrecord...:   0% 0/2000 [00:00<?, ? examples/s]\u001b[A\n","INFO[tfrecords_writer.py]: Done writing cifar35200-test.tfrecord. Number of examples: 2000 (shards: [2000])\n","\u001b[1mDataset cifar35200 downloaded and prepared to /root/tensorflow_datasets/cifar35200/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","INFO[build.py]: Dataset generation complete...\n","\n","tfds.core.DatasetInfo(\n","    name='cifar35200',\n","    full_name='cifar35200/1.0.0',\n","    description=\"\"\"\n","    \n","    \"\"\",\n","    homepage='https://www.tensorflow.org/datasets/catalog/cifar35200',\n","    data_path='/root/tensorflow_datasets/cifar35200/1.0.0',\n","    download_size=Unknown size,\n","    dataset_size=135.50 MiB,\n","    features=FeaturesDict({\n","        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n","        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n","    }),\n","    supervised_keys=('image', 'label'),\n","    disable_shuffling=False,\n","    splits={\n","        'test': <SplitInfo num_examples=2000, num_shards=1>,\n","        'train': <SplitInfo num_examples=50000, num_shards=1>,\n","        'valid': <SplitInfo num_examples=10000, num_shards=1>,\n","    },\n","    citation=\"\"\"\"\"\",\n",")\n","\n"]}]},{"cell_type":"code","metadata":{"id":"f0gefc6G-A1G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674771308,"user_tz":300,"elapsed":950,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"b84d1395-4451-401a-a4f8-adf138793267"},"source":["# mini test\n","%cd ~\n","\n","import tensorflow_datasets as tfds\n","\n","ds = tfds.load('cifar35200')\n","ds"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]},{"output_type":"execute_result","data":{"text/plain":["{'test': <PrefetchDataset shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n"," 'train': <PrefetchDataset shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n"," 'valid': <PrefetchDataset shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"zFEEKJNVPbEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674771309,"user_tz":300,"elapsed":5,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"3f8fffd6-95e4-484e-e60e-53bee163e9d4"},"source":["len(ds['valid'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"Mnx29rw1Xr2a"},"source":["### Dataset Preprocessing via Transform"]},{"cell_type":"code","metadata":{"id":"1bZtnVYK0YiX"},"source":["import tensorflow.image as transforms\n","import tensorflow_addons as tfa\n","import tensorflow_datasets as tfds\n","\n","from pathlib import Path\n","\n","# don't forget the as_supervised=True, otherwise transformation functions can't find 'image' and 'label' correctly\n","trainset, validset, testset = tfds.load('cifar35200', as_supervised=True, split=['train', 'valid', 'test'])\n","\n","## these are the pytorch transforms. I tried to mimic its behavior, but not exactly the same due to missing functions\n","# transform_train = transforms.Compose([transforms.Resize((32,32)),  #resises the image so it can be perfect for our model.\n","#                                       transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n","#                                       transforms.RandomRotation(10),     #Rotates the image to a specified angel in degrees\n","#                                       transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n","#                                       transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n","#                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize all the images\n","#                                       ])\n"," \n","# transform = transforms.Compose([transforms.Resize((32,32)),\n","#                                transforms.ToTensor(),\n","#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","#                                ])\n","\n","def transform_train(image, label):\n","    image = transforms.resize(image, (32, 32))\n","    image = transforms.random_flip_left_right(image)\n","    image = tfa.image.transform_ops.rotate(image, 0.174533) # angle in radians\n","    image = transforms.random_contrast(image, 0.8, 1.2)\n","    image = transforms.random_saturation(image, 0.8, 1.2)\n","    image = transforms.random_brightness(image, 0.2)\n","    image = transforms.per_image_standardization(image)\n","    return image, label\n","\n","def transform(image, label):\n","    image = transforms.resize(image, (32, 32))\n","    image = transforms.per_image_standardization(image)\n","    return image, label\n","\n","trainset = trainset.map(transform_train).shuffle(10000).batch(BATCH_SIZE)\n","validset = validset.map(transform).batch(BATCH_SIZE)\n","testset = testset.map(transform).batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNhomSOw0-2Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633674779975,"user_tz":300,"elapsed":87,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"19407241-8ada-4fdf-cc3d-47f5413a2005"},"source":["trainset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((None, 32, 32, 3), (None,)), types: (tf.float32, tf.int64)>"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"xwSO-GomvQpl"},"source":["## Define Model"]},{"cell_type":"code","metadata":{"id":"MbqusIzKnGOs"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D\n","from tensorflow.keras import Model\n","\n","class LeNet(Model):\n","  def __init__(self):\n","    super(LeNet, self).__init__()\n","    self.conv1 = Conv2D(16, 3, activation='relu')\n","    self.conv2 = Conv2D(32, 3, activation='relu')\n","    self.conv3 = Conv2D(63, 3, activation='relu')\n","    self.dropout1 = Dropout(0.5)\n","    self.flatten = Flatten()\n","    self.maxpool = MaxPool2D((2,2), 2)\n","    self.fc1 = Dense(500, activation='relu')\n","    self.fc2 = Dense(10)\n","\n","  def call(self, x):\n","    x = self.conv1(x)\n","    x = self.maxpool(x)\n","    x = self.conv2(x)\n","    x = self.maxpool(x)\n","    x = self.conv3(x)\n","    x = self.maxpool(x)\n","    x = self.flatten(x)\n","    x = self.fc1(x)\n","    x = self.dropout1(x)\n","    x = self.fc2(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfMrklEJx9mM"},"source":["# Create an instance of the model\n","model = LeNet()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-HXMJYZfELuR"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"6u42lK4iEdn4"},"source":["learning_rate = 0.001\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AX4QXjBNEjfg"},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n","valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4BoJfLaElLQ"},"source":["@tf.function\n","def train_step(images, labels):\n","  with tf.GradientTape() as tape:\n","    # training=True is only needed if there are layers with different\n","    # behavior during training versus inference (e.g. Dropout).\n","    predictions = model(images, training=True)\n","    loss = loss_object(labels, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)\n","\n","@tf.function\n","def valid_step(images, labels):\n","  # training=False is only needed if there are layers with different\n","  # behavior during training versus inference (e.g. Dropout).\n","  predictions = model(images, training=False)\n","  v_loss = loss_object(labels, predictions)\n","\n","  valid_loss(v_loss)\n","  valid_accuracy(labels, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPIZoc1gEm31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633675242612,"user_tz":300,"elapsed":404525,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"1dc966e2-e729-40a2-b669-b1eeacc4f239"},"source":["from tqdm import tqdm\n","\n","for epoch in range(EPOCHS):\n","    # Reset the metrics at the start of the next epoch\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","    valid_loss.reset_states()\n","    valid_accuracy.reset_states()\n","\n","    for images, labels in tqdm(trainset):\n","        train_step(images, labels)\n","    for images, labels in validset:\n","        valid_step(images, labels)\n","\n","    print('Epoch: %d | Train Loss: %.4f | Train Accuracy: %.4f | Validation Loss: %.4f | Validation Accuracy: %.4f' \\\n","          %(epoch, train_loss.result(), train_accuracy.result() * 100, valid_loss.result(), valid_accuracy.result() * 100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:27<00:00, 56.72it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Train Loss: 1.5015 | Train Accuracy: 45.5320 | Validation Loss: 1.4379 | Validation Accuracy: 50.9700\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 67.00it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Train Loss: 1.1683 | Train Accuracy: 58.4900 | Validation Loss: 1.2992 | Validation Accuracy: 55.0600\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.74it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Train Loss: 1.0400 | Train Accuracy: 63.5520 | Validation Loss: 1.3360 | Validation Accuracy: 56.3200\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.82it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 | Train Loss: 0.9583 | Train Accuracy: 66.3480 | Validation Loss: 1.2760 | Validation Accuracy: 59.7300\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.32it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 4 | Train Loss: 0.8954 | Train Accuracy: 68.7040 | Validation Loss: 1.2376 | Validation Accuracy: 61.1800\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.23it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 5 | Train Loss: 0.8475 | Train Accuracy: 70.2480 | Validation Loss: 1.1617 | Validation Accuracy: 62.6100\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 67.06it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 6 | Train Loss: 0.8026 | Train Accuracy: 71.7680 | Validation Loss: 1.1669 | Validation Accuracy: 63.4100\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:40<00:00, 38.13it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 7 | Train Loss: 0.7643 | Train Accuracy: 72.9680 | Validation Loss: 1.1309 | Validation Accuracy: 64.3700\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 67.84it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 8 | Train Loss: 0.7323 | Train Accuracy: 73.9940 | Validation Loss: 1.2461 | Validation Accuracy: 63.1600\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.85it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 9 | Train Loss: 0.7106 | Train Accuracy: 74.8860 | Validation Loss: 1.3125 | Validation Accuracy: 62.5100\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 67.69it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 10 | Train Loss: 0.6780 | Train Accuracy: 75.9260 | Validation Loss: 1.2305 | Validation Accuracy: 64.3200\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.74it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 11 | Train Loss: 0.6582 | Train Accuracy: 76.5460 | Validation Loss: 1.3305 | Validation Accuracy: 63.2800\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.64it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 12 | Train Loss: 0.6359 | Train Accuracy: 77.3220 | Validation Loss: 1.2663 | Validation Accuracy: 64.0600\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 67.25it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 13 | Train Loss: 0.6170 | Train Accuracy: 78.0900 | Validation Loss: 1.3274 | Validation Accuracy: 63.9600\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1563/1563 [00:23<00:00, 66.78it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 14 | Train Loss: 0.5966 | Train Accuracy: 78.7260 | Validation Loss: 1.2868 | Validation Accuracy: 63.9000\n"]}]},{"cell_type":"markdown","metadata":{"id":"ee8KoPUaW0ro"},"source":["## Test score"]},{"cell_type":"code","metadata":{"id":"Jb66Wi4qW0Nm"},"source":["@tf.function\n","def test_step(images, labels):\n","  # training=False is only needed if there are layers with different\n","  # behavior during training versus inference (e.g. Dropout).\n","  predictions = model(images, training=False)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss(t_loss)\n","  test_accuracy(labels, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fqIQZQ11-LM","executionInfo":{"status":"ok","timestamp":1633675661500,"user_tz":300,"elapsed":1362,"user":{"displayName":"Peng Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGl3Vbo0NnPdfsa69kLBgTNmsh3kz0ri5e4y5a=s64","userId":"08765295497597902326"}},"outputId":"38d8d4f5-53ab-4a24-d8c4-e1c07c1c1a60"},"source":["for test_images, test_labels in testset:\n","    test_step(test_images, test_labels)\n","\n","print(f'Test Accuracy: {test_accuracy.result() * 100:.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 52.15\n"]}]}]}